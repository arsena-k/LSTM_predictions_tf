{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An LSTM Neural Net to predict depression, or depression symptoms based on the PHQ-8 from transcribed verbal data (utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this code is exploratory, may contain errors!\n",
    "* python 2 with Tensorflow\n",
    "* prediction options include: depression level (continuous, low/med/high, or binary), level for each depression symptom based on PHQ-8 (low/med/high, or binary). Need to change the cost function in the code depending on the outcome chosen. \n",
    "* data is unbalanced, so options in code for 1) undersampling the majority class (lower levels of depression) and 2) cost-senstive learning\n",
    "* various options for drop out, L1/L2 regaularization\n",
    "\n",
    "Unique features of this model compared to other frameworks explored:\n",
    "* includes embedding layer prior to LSTM layer. Here, the emebddings are learned along the way not pre-trained. \n",
    "* set up for mulit-task learning, to try predicting multiple outcomes (e.g., predict two symptoms rather than just one). The goal of this is to improve generalizability of the model. \n",
    "* sequences of various lengths may be used, or a sliding window of words (to increase sample size and variability of utterances)\n",
    "* debug_sentences allows us to write the sentences in each part of the confusion matrix to a file for later examination of the erors\n",
    "\n",
    "\n",
    "Alina notes to self:\n",
    "* data corresponding to IDs without meta-information and data corresponding to missing target information are excluded in Load_Data()\n",
    "* for variable coding see codebook excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Helpful tutorials:\n",
    "* https://github.com/nfmcclure/tensorflow_cookbook/blob/master/09_Recurrent_Neural_Networks/02_Implementing_RNN_for_Spam_Prediction/02_implementing_rnn.py\n",
    "* Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf; Author: Aymeric Damien; Project: https://github.com/aymericdamien/TensorFlow-Examples/    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import sys\n",
    "import csv\n",
    "from collections import deque\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cwd= os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==========\n",
    "#   MODEL\n",
    "# Some LSTM code and code structure reused from M. Morales\n",
    "# ==========\n",
    "class BaseConfiguration:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.0001 #also try .001, i think accuracy and confusion matrix could be more stable with lower learning rate\n",
    "        self.training_iters = 3000000  #was 50,000,000. Consider reducing further if you notice that the testing accuracy becomes stable (or starts dropping) earlier than this number of iterations, to prevent overfitting. \n",
    "        self.batch_size = 50\n",
    "        self.display_step = 1\n",
    "        self.val_step = 50\n",
    "        self.L2_penalty= 0.4 #beta term on L2 weight penalty for L2 normalization, if set to 0.0 for no L2 normalization. I think confusion matrix becomes unbalanced (but accuracy higher) with L2 noamlization\n",
    "        # Network Parameters\n",
    "        self.seq_max_len = 115 # Sequence max length. In the case of rolling windows, this is also the size of the window (i.e., how many words are in a window)\n",
    "        self.seq_min_len = 7\n",
    "        self.embedding_size = 30\n",
    "        self.n_hidden = 30 # hidden layer num of features, per layer. 15 seems to small, doesn't learn beyond 50%\n",
    "        self.n_classes = 2 # linear sequence or not\n",
    "        self.num_layers = 1 #keep at 1 for now, should 2x check the function before adding more than 1 layer. #https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "        self.keep_prob = .9 #for no dropout, use 1. This only performs dropout on the LSTM layer, for inputs AND outputs outputs. If config.n_layers>1, dropout will work on every layer, i.e., if set dropout to .9 and 2 layers, total dropout is ,.9*.9\n",
    "        self.debug_sentences = False #if True, sentences corresponding to each cell in the confusion matrix are written in folder \"sentence_validation\"\n",
    "        self.min_word_frequency=15 #wont work well below 15\n",
    "        self.validation_IDs_proportion=.15 #proportion of IDs (among IDs that exist in meta data, and have the specified target value) that are held out as \"validation\" data\n",
    "        self.balance_classes=True #use for balancing classification with undersampling. If True, will only sample the number of the minority category utterances to be same as high utterances (for training data)\n",
    "        self.step_windows=1 #the step (amt of words to jump) to get to the next rolling windows.  \n",
    "        self.toss=75 #skip first config.toss words in each transcript to get over small-talk. Best between 50-75 number after reading a few transcripts, could try firsrt 75 or 100 words even\n",
    "\n",
    "    def printConfiguration(self):\n",
    "        # print configuration\n",
    "        print ('---------- Configuration: ----------')\n",
    "        print ('learning_rate', self.learning_rate)\n",
    "        print ('training_iters', self.training_iters)\n",
    "        print ('batch_size', self.batch_size) \n",
    "        print ('L2_penalty', self.L2_penalty) \n",
    "\n",
    "        # Network Parameters\n",
    "        print ('seq_max_len', self.seq_max_len )# Sequence max length\n",
    "        print ('seq_min_len', self.seq_min_len)\n",
    "        print ('embedding_size', self.embedding_size)\n",
    "        print ('n_hidden', self.n_hidden) # hidden layer num of features\n",
    "        print ('n_classes', self.n_classes) # linear sequence or not\n",
    "        print ('num_layers', self.num_layers)\n",
    "        print ('keep_prob (dropout = 1-keep_prob)', self.keep_prob)\n",
    "        print ('------------------------------------')\n",
    "\t#print 'n_hidden: ', self.n_hidden, 'learning rate: ', self.learning_rate, ' batch_size: ', self.batch_size, ' max_len: ', self.seq_max_len, ' min_len: ', self.seq_min_len\n",
    "\n",
    "# Parameters\n",
    "# configuration\n",
    "config = BaseConfiguration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #to reset tensorflow graph if you've already made one graph, once re-set need to run through to get variable agani. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Tensorflow Graph Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "#x = tf.placeholder(tf.float32, [None, config.seq_max_len]) #this plaecholder for x used if pre-trained embeddings are used\n",
    "y1 = tf.placeholder(tf.float32, [None, config.n_classes])\n",
    "x = tf.placeholder(tf.int32, [None, config.seq_max_len]) #this placeholder for x used if training embedding layer\n",
    "y2= tf.placeholder(tf.float32, [None, config.n_classes])\n",
    "\n",
    "#x = tf.placeholder(tf.float32, [None, config.seq_max_len, config.embedding_size]) #Number of examples, number of input, dimension of each input\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Define weights\n",
    "weights1 = {\n",
    "    'out': tf.Variable(tf.random_normal([config.n_hidden, config.n_classes], seed= 43)) #note weights are initialized as normal random variable\n",
    "}\n",
    "biases1 = {\n",
    "    'out': tf.Variable(tf.random_normal([config.n_classes], seed= 43))  #note biases are initialized as normal random variable\n",
    "}\n",
    "\n",
    "\n",
    "weights2 = {\n",
    "    'out': tf.Variable(tf.random_normal([config.n_hidden, config.n_classes], seed= 43)) #note weights are initialized as normal random variable\n",
    "}\n",
    "biases2 = {\n",
    "    'out': tf.Variable(tf.random_normal([config.n_classes], seed= 43))  #note biases are initialized as normal random variable\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(x, y1, y2,  sequence_length, part_size=0, sentences=None):\n",
    "    batch_x = []\n",
    "    batch_y1 = []\n",
    "    batch_y2 = []\n",
    "    batch_sequence_length = []\n",
    "    batch_sentences = []\n",
    "    indexes = range(len(x))\n",
    "    random.shuffle(indexes)\n",
    "    #print(indexes[0:batch_size-1])\n",
    "    for index in indexes[0:part_size-1]:\n",
    "        batch_x.append(x[index])\n",
    "        batch_y1.append(y1[index])\n",
    "        batch_y2.append(y2[index])\n",
    "        batch_sequence_length.append(sequence_length[index])\n",
    "        if sentences is not None:\n",
    "            batch_sentences.append(sentences[index])\n",
    "    return batch_x, batch_y1, batch_y2, batch_sequence_length, batch_sentences\n",
    "\n",
    "def last_relevant(output, length):\n",
    "    config.batch_size = tf.shape(output)[0]\n",
    "    max_length = int(output.get_shape()[1])\n",
    "    output_size = int(output.get_shape()[2])\n",
    "    index = tf.range(0, config.batch_size) * max_length + (length - 1)\n",
    "    flat = tf.reshape(output, [-1, output_size])\n",
    "    relevant = tf.gather(flat, index)\n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_meta_data(path):\n",
    "    \"\"\"Imports labels as a dictionary- must designate below which label you want\"\"\"\n",
    "    labels = {}\n",
    "    # load the CSV file as a numpy matrix\n",
    "    with open (path,'r') as csv:\n",
    "        dataset = csv.readlines()\n",
    "    for row in dataset[1:]:\n",
    "        ID, depression, dep_binary2, depression_value, depression_level_official, Morethan7PHQsympt_Available,PHQ_9NoInterest,PHQ_9Depressed,PHQ_9Sleep,PHQ_9Tired,PHQ_9Appetite,PHQ_9Failure,PHQ_9Concentrating,PHQ_9Moving,PHQ_9NoInterest_level,PHQ_9Depressed_level,PHQ_9Sleep_level,PHQ_9Tired_level,PHQ_9Appetite_level,PHQ_9Failure_level,PHQ_9Concentrating_level,PHQ_9Moving_level  = row.strip().split(',')\n",
    "        #map depression strings to int values\n",
    "        depression_level_official = depression_level_official.strip()\n",
    "        PHQ_9NoInterest_level = PHQ_9NoInterest_level.strip()\n",
    "        PHQ_9Depressed_level = PHQ_9Depressed_level.strip()\n",
    "        PHQ_9Sleep_level = PHQ_9Sleep_level.strip()\n",
    "        PHQ_9Tired_level = PHQ_9Tired_level.strip()\n",
    "        PHQ_9Appetite_level = PHQ_9Appetite_level.strip()\n",
    "        PHQ_9Failure_level = PHQ_9Failure_level.strip()\n",
    "        PHQ_9Concentrating_level = PHQ_9Concentrating_level.strip()\n",
    "        PHQ_9Moving_level = PHQ_9Moving_level.strip()\n",
    "        \n",
    "        if config.n_classes==2:\n",
    "            if depression_level_official=='low':\n",
    "                depression_level_official = 0\n",
    "            elif depression_level_official=='mid':\n",
    "                depression_level_official = 1\n",
    "            elif depression_level_official=='high':\n",
    "                depression_level_official =1\n",
    "        \n",
    "        if config.n_classes==3:\n",
    "            if depression_level_official=='low':\n",
    "                depression_level_official = 0\n",
    "            elif depression_level_official=='mid':\n",
    "                depression_level_official = 1\n",
    "            elif depression_level_official=='high':\n",
    "                depression_level_official =2\n",
    "        \n",
    "        if dep_binary2=='0': #need to do this because there are NAs in the data, so it is loaded in as a string not number\n",
    "            dep_binary2=0\n",
    "        elif dep_binary2=='1':\n",
    "            dep_binary2=1\n",
    "            \n",
    "        if PHQ_9NoInterest_level=='low':\n",
    "            PHQ_9NoInterest_level = 0\n",
    "        elif PHQ_9NoInterest_level=='mid':\n",
    "            PHQ_9NoInterest_level = 1\n",
    "        elif PHQ_9NoInterest_level=='high':\n",
    "            if config.n_classes==2:\n",
    "                PHQ_9NoInterest_level =1\n",
    "            elif config.n_classes==3:\n",
    "                PHQ_9NoInterest_level =2    \n",
    "       \n",
    "        if PHQ_9Depressed_level=='low':\n",
    "            PHQ_9Depressed_level = 0\n",
    "        elif PHQ_9Depressed_level=='mid':\n",
    "            PHQ_9Depressed_level = 1\n",
    "        elif PHQ_9Depressed_level=='high':\n",
    "            if config.n_classes==2:\n",
    "                PHQ_9Depressed_level =1\n",
    "            elif config.n_classes==3:\n",
    "                PHQ_9Depressed_level =2     \n",
    "            \n",
    "        if PHQ_9Sleep_level=='low':\n",
    "            PHQ_9Sleep_level = 0\n",
    "        elif PHQ_9Sleep_level=='mid':\n",
    "            PHQ_9Sleep_level = 1\n",
    "        elif PHQ_9Sleep_level=='high':\n",
    "            if config.n_classes==2:\n",
    "                PHQ_9Sleep_level =1\n",
    "            elif config.n_classes==3:\n",
    "                PHQ_9Sleep_level =2     \n",
    "    \n",
    "        if PHQ_9Tired_level=='low':\n",
    "            PHQ_9Tired_level = 0\n",
    "        elif PHQ_9Tired_level=='mid':\n",
    "            PHQ_9Tired_level = 1\n",
    "        elif PHQ_9Tired_level=='high':\n",
    "            if config.n_classes==2:\n",
    "                PHQ_9Tired_level =1\n",
    "            elif config.n_classes==3:\n",
    "                PHQ_9Tired_level =2     \n",
    "            \n",
    "        if PHQ_9Appetite_level=='low':\n",
    "            PHQ_9Appetite_level = 0\n",
    "        elif PHQ_9Appetite_level=='mid':\n",
    "            PHQ_9Appetite_level = 1\n",
    "        elif PHQ_9Appetite_level=='high':\n",
    "            if config.n_classes==2:\n",
    "                PHQ_9Appetite_level =1\n",
    "            elif config.n_classes==3:\n",
    "                PHQ_9Appetite_level=2      \n",
    "            \n",
    "        if PHQ_9Failure_level=='low':\n",
    "            PHQ_9Failure_level = 0\n",
    "        elif PHQ_9Failure_level=='mid':\n",
    "            PHQ_9Failure_level = 1\n",
    "        elif PHQ_9Failure_level=='high':\n",
    "            if config.n_classes==2:\n",
    "                PHQ_9Failure_level =1\n",
    "            elif config.n_classes==3:\n",
    "                PHQ_9Failure_level =2      \n",
    "            \n",
    "        if PHQ_9Concentrating_level=='low':\n",
    "            PHQ_9Concentrating_level = 0\n",
    "        elif PHQ_9Concentrating_level=='mid':\n",
    "            PHQ_9Concentrating_level = 1\n",
    "        elif PHQ_9Concentrating_level=='high':\n",
    "            if config.n_classes==2:\n",
    "                PHQ_9Concentrating_level =1\n",
    "            elif config.n_classes==3:\n",
    "                PHQ_9Concentrating_level =2     \n",
    "        \n",
    "        if PHQ_9Moving_level=='low':\n",
    "            PHQ_9Moving_level = 0\n",
    "        elif PHQ_9Moving_level=='mid':\n",
    "            PHQ_9Moving_level = 1\n",
    "        elif PHQ_9Moving_level=='high':\n",
    "            if config.n_classes==2:\n",
    "                PHQ_9Moving_level =1\n",
    "            elif config.n_classes==3:\n",
    "                PHQ_9Moving_level =2     \n",
    "        \n",
    "        labels[ID.strip()] = [depression, dep_binary2, depression_value, depression_level_official,PHQ_9NoInterest_level,PHQ_9Depressed_level,PHQ_9Sleep_level,PHQ_9Tired_level,PHQ_9Appetite_level,PHQ_9Failure_level,PHQ_9Concentrating_level,PHQ_9Moving_level]\n",
    "    return labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#meta_data = import_meta_data('Meta_Data_with_Symptoms.csv')\n",
    "#meta_data['311']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Transcript Data, to eventually reshape into moving windows of utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.hist(lengths, 300, normed=1, facecolor='green', alpha=0.75)\\nplt.xlabel('Transcript Length')\\nplt.ylabel('Probability')\\nplt.title('Distribution of Transcript Lengths')\\nplt.show()\\n\\n#statistics on transcript lengths\\nimport statistics\\nfrom fractions import Fraction as F\\nfrom decimal import Decimal as D\\n\\nlow=0\\nprint(statistics.mean(lengths), statistics.median(lengths), min(lengths), max(lengths))\\nfor i in lengths:\\n    if i<200:\\n        low=low+1\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_path= cwd + '/REcleaned_utterances_data_July2017/Cleaned_Utterances_by_ID/'\n",
    "\n",
    "#load utterances data\n",
    "utterances=[]\n",
    "IDs_list= []\n",
    "\n",
    "files= os.listdir(data_file_path)\n",
    "#files= ['Utterances771.txt', 'Utterances300.txt']\n",
    "lengths=[]\n",
    "\n",
    "for ppt_file in files:\n",
    "    path= data_file_path + str(ppt_file)\n",
    "    f = open(path,'r')\n",
    "    lines =  [line.strip() for line in f.readlines()]\n",
    "    lines= \" \".join(lines)\n",
    "    lines= lines.split()\n",
    "    if len(lines) > ((config.toss+config.seq_max_len) -1):\n",
    "        skip=config.toss\n",
    "        utterances.append(lines[skip:]) #skip first 'toss' words to get over small-talk. chose this number after reading a few transcripts, could try firsrt 75 or 100 words even\n",
    "        ID=re.sub('Utterances', '', ppt_file)\n",
    "        ID=re.sub('.txt', '', ID)\n",
    "        IDs_list.append(ID)\n",
    "        lengths.append(len(lines))\n",
    "    f.close()\n",
    "\n",
    "#Plot distribution of transcript lengths\n",
    "'''\n",
    "plt.hist(lengths, 300, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.xlabel('Transcript Length')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Distribution of Transcript Lengths')\n",
    "plt.show()\n",
    "\n",
    "#statistics on transcript lengths\n",
    "import statistics\n",
    "from fractions import Fraction as F\n",
    "from decimal import Decimal as D\n",
    "\n",
    "low=0\n",
    "print(statistics.mean(lengths), statistics.median(lengths), min(lengths), max(lengths))\n",
    "for i in lengths:\n",
    "    if i<200:\n",
    "        low=low+1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Transcript Data into moving windows of utterances. Window size and jump between windows is set in config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adjusted from: https://stackoverflow.com/questions/6822725/rolling-or-sliding-window-iterator-in-python\n",
    "def sliding_window(iterable, size, step, fillvalue=' '):\n",
    "    if size < 0 or step < 1:\n",
    "        raise ValueError\n",
    "    it = iter(iterable) #list of the words in the transcript\n",
    "    q = deque(islice(it, size), maxlen=size)\n",
    "    if not q:\n",
    "        return  # empty iterable or size == 0\n",
    "    q.extend(fillvalue for _ in range(size - len(q)))  # pad to size\n",
    "    while True:\n",
    "        yield iter(q)  # iter() to avoid accidental outside modifications\n",
    "        try:\n",
    "            q.append(next(it))\n",
    "        except StopIteration: # Python 3.5 pep 479 support\n",
    "            return\n",
    "        q.extend(next(it) for _ in range(step - 1))\n",
    "        \n",
    "'''Testing:\n",
    "\n",
    "utts=[]\n",
    "for i in sliding_window(trial, 10, 5):\n",
    "    utts.append(list(i))\n",
    "\n",
    "trial='um my family moved to the u_s and then i moved down here eventually for college uh it took a long time to ive been living here'\n",
    "trial=trial.split( )\n",
    "len(trial)\n",
    "\n",
    "for i in utts:\n",
    "    print(i)\n",
    "'''\n",
    "utterances_seqs=[]\n",
    "for i in range(0,len(utterances)):\n",
    "    for j in sliding_window(utterances[i], config.seq_max_len, config.step_windows):\n",
    "        listy= ' '.join(list(j))\n",
    "        listy= listy + ',' + str(IDs_list[i])\n",
    "        listy= ''.join(listy).split(',')\n",
    "        #utts.append(list(j)+ [str(IDs[i])])\n",
    "        #utts.append(IDs[i]) #now utterances_seqs[][-1] is ID number\n",
    "        utterances_seqs.append(listy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for merging utterance window data and meta, and shaping for the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_meta=[] #this is just for inspecting IDs that aren't in the meta-data file, and so not used in RNN code\n",
    "IDs_NAs_inMeta=[] #this is just for inspecting IDs that are in the meta-data file but don't have target values, so not used in RNN code\n",
    "\n",
    "def load_data(target1=None, target2= None, meta_file_path='Meta_Data_with_Symptoms.csv', LIWC_file_path=cwd + '/LIWC/LIWC_features_per_ID.csv' ):\n",
    "\n",
    "    if target1 is None:\n",
    "        print(\"Target not specified: possible target classes: depression_binary=0, dep_binary2=1, depression value=2, depression_level_official=3, PHQ_9NoInterest_level=4,PHQ_9Depressed_level=5,PHQ_9Sleep_level=6,PHQ_9Tired_level=7,PHQ_9Appetite_level=8,PHQ_9Failure_level=9,PHQ_9Concentrating_level=10,PHQ_9Moving_level=11\")\n",
    "        raise\n",
    "    #get labels\n",
    "    meta_data = import_meta_data(meta_file_path)\n",
    "    \n",
    "    \n",
    "    #training data variables\n",
    "    utterances_withIDs = [] #utterances of training data with meta data (ID of speaker, line in transcript, and quarter of transcript)\n",
    "    data_y1 = [] #this will be an array with an element for each sentence in the training data, with the depression label, in the format [X,X,X] \n",
    "    data_y2= []\n",
    "    sequence_length = [] #vector of sequence lengths for each utterances in the training data\n",
    "    sentences = [] #just the utterances of training data, no meta data included \n",
    "    Nlow_utterances=[] #number of utterances from those in training category with low depression, used to balance cost function\n",
    "    Nmid_utterances=[] #number of utterances from those in training category with mid depression, used to balance cost function\n",
    "    Nhigh_utterances=[] #number of utterances from those in training category with high depression, used to balance cost function\n",
    "    IDs_training= []\n",
    "    \n",
    "    #validation data variables\n",
    "    utterances_validation=[]  #utterances of validation data with meta data (ID of speaker, line in transcript, and quarter of transcript)\n",
    "    validation_y1 =[]  #this will be an array with an element for each sentence in the validation data, with the depression label, in the format [X,X,X] \n",
    "    validation_y2 =[]\n",
    "    sequence_length_validation=[] #vector of sequence lengths for each utterances in the training data\n",
    "    sentences_validation=[] #just the utterances of validation data, no meta data included \n",
    "    IDs_validation=[] #list of validation IDs so that validation ID's are excluded from training data, and it is in same order as the utterances, sentences, sequence length, y, etc. \n",
    "\n",
    "\n",
    "    #get validation IDs, and list them in validation_IDs_set\n",
    "    IDs_noNAs_inMeta=[] #list of IDs in sentences that have corresponding meta data and have the specified target value\n",
    "    for L in utterances_seqs:\n",
    "        ID = L[1].strip() \n",
    "        try:\n",
    "            meta = meta_data[ID]\n",
    "            if meta[target1] != 'NA':\n",
    "                if meta[target2] != 'NA':\n",
    "                    IDs_noNAs_inMeta.append(ID)\n",
    "            else:\n",
    "                IDs_NAs_inMeta.append(ID) #just in case want to track who was excluded\n",
    "        except:\n",
    "            missing_meta.append(ID)\n",
    "            continue       \n",
    "    validation_IDs_set= random.sample(set(IDs_noNAs_inMeta), int(round(config.validation_IDs_proportion*len(set(IDs_noNAs_inMeta))))) #get number of IDs for validation set based on the proportion set in the configuration, round the number to nearest integer\n",
    "    #fill in variables for utterances (i.e, sentences with meta info), sentences, IDs, and sequence lengths (for training and validation sets separately)\n",
    "    utterances_withIDs_low=[] #this can be used to sample among utterances for \"low\" depression\n",
    "    utterances_withIDs_mid=[] #this can be used to sample among utterances for \"mid\" depression\n",
    "\n",
    "    for L in utterances_seqs:\n",
    "        ID = L[1].strip()    \n",
    "        length = len(L[0].split(\" \")) #each line is for example: ['me there at all can you do it at home', ' 602', '1', 'first quarter'], so the 0th item is the words, this is measuring characters, if want to measure words split here split.(\"\\\\s+\")\n",
    "        if length > config.seq_max_len: #truncate at max sequence length\n",
    "            length=config.seq_max_len\n",
    "        minimum_seqs= config.seq_min_len-1 #such that the minimum length in the next line is inclusive of the minimum number itself (e.g. if minimum sequence length is 6 words then sequences of 6 will be included)\n",
    "        \n",
    "        if length > minimum_seqs:\n",
    "            try:\n",
    "                meta = meta_data[ID]\n",
    "                if meta[target1] == 0: #exclude participants without data for the specific target\n",
    "                    if ID in IDs_noNAs_inMeta: #IDs in validation set are EXCLUDED from variables below\n",
    "                        if ID not in validation_IDs_set:\n",
    "                            utterances_withIDs_low.append(L) # can sample among these,below #this is a list of [utterance, id] for each utterance. but only includes those corresponding to indivduals with valid ids\n",
    "                        elif ID in validation_IDs_set:\n",
    "                            sentences_validation.append(L[0]) \n",
    "                            utterances_validation.append(L) \n",
    "                            IDs_validation.append(ID)\n",
    "                            sequence_length_validation.append(length)\n",
    "                elif meta[target1] ==1: #exclude participants without data for the specific target\n",
    "                    if ID in IDs_noNAs_inMeta: \n",
    "                        if ID not in validation_IDs_set: #IDs in validation set are EXCLUDED from variables below\n",
    "                            utterances_withIDs_mid.append(L) #this is a list of [utterance, id] for each utterance. but only includes those corresponding to indivduals with valid ids\n",
    "                        elif ID in validation_IDs_set:\n",
    "                            sentences_validation.append(L[0]) \n",
    "                            utterances_validation.append(L) \n",
    "                            IDs_validation.append(ID)\n",
    "                            sequence_length_validation.append(length)\n",
    "                elif meta[target1] ==2: #exclude participants without data for the specific target\n",
    "                     if ID in IDs_noNAs_inMeta: \n",
    "                        if ID not in validation_IDs_set: #IDs in validation set are EXCLUDED from variables below\n",
    "                            sentences.append(L[0]) #now this is a list of utterances coresponding to indivdiuals with valid IDs\n",
    "                            utterances_withIDs.append(L) #this is a list of [utterance, id] for each utterance. but only includes those corresponding to indivduals with valid ids\n",
    "                            IDs_training.append(ID)\n",
    "                            sequence_length.append(length)\n",
    "                        elif ID in validation_IDs_set:\n",
    "                            sentences_validation.append(L[0]) \n",
    "                            utterances_validation.append(L) \n",
    "                            IDs_validation.append(ID)\n",
    "                            sequence_length_validation.append(length)\n",
    "            except:\n",
    "                #print('invalid ID: ', ID)\n",
    "                continue\n",
    "    \n",
    "    #this is where the \"low\" (for 2 or 3 class predictions) and \"mid\" (for 3 class predictions) level depression setences may be undersampled\n",
    "    if config.balance_classes==True:\n",
    "        if config.n_classes==2:\n",
    "            try:\n",
    "                #try this, will throw and exception for a few PHQ symptoms which are arleady balanced enough and have more mid than low\n",
    "                utterances_withIDs_low= random.sample(utterances_withIDs_low, len(utterances_withIDs_mid)) #for three classes not right sampling\n",
    "            except:\n",
    "                pass\n",
    "        elif config.n_classes==3:\n",
    "            utterances_withIDs_low= random.sample(utterances_withIDs_low, len(utterances_withIDs)) #for three classes not right sampling\n",
    "            utterances_withIDs_mid= random.sample(utterances_withIDs_mid, len(utterances_withIDs)) #for three classes not right sampling\n",
    "        \n",
    "    for L in utterances_withIDs_low:\n",
    "        ID = L[1].strip()    \n",
    "        length = len(L[0].split(\" \")) #each line is for example: ['me there at all can you do it at home', ' 602', '1', 'first quarter'], so the 0th item is the words, this is measuring characters, if want to measure words split here split.(\"\\\\s+\")\n",
    "        if length > config.seq_max_len: #truncate at max sequence length\n",
    "            length=config.seq_max_len\n",
    "        \n",
    "        sentences.append(L[0]) #now this is a list of utterances coresponding to indivdiuals with valid IDs\n",
    "        utterances_withIDs.append(L) \n",
    "        IDs_training.append(ID)\n",
    "        sequence_length.append(length)\n",
    "    \n",
    "    for L in utterances_withIDs_mid:\n",
    "        ID = L[1].strip()    \n",
    "        length = len(L[0].split(\" \")) #each line is for example: ['me there at all can you do it at home', ' 602', '1', 'first quarter'], so the 0th item is the words, this is measuring characters, if want to measure words split here split.(\"\\\\s+\")\n",
    "        if length > config.seq_max_len: #truncate at max sequence length\n",
    "            length=config.seq_max_len\n",
    "        \n",
    "        sentences.append(L[0]) #now this is a list of utterances coresponding to indivdiuals with valid IDs\n",
    "        utterances_withIDs.append(L) \n",
    "        IDs_training.append(ID)\n",
    "        sequence_length.append(length)\n",
    "    #fill in re-shaped labels variables (for training and validation separately, and do depending on the number of classes- 2 or 3)\n",
    "    if config.n_classes==2:\n",
    "        for L in utterances_withIDs:\n",
    "            ID = L[1].strip()\n",
    "            meta = meta_data[ID]\n",
    "            if meta[target1]==0:\n",
    "                data_y1.append([1.0, 0.0])\n",
    "                Nlow_utterances.append(1)\n",
    "            elif meta[target1] == 1:\n",
    "                data_y1.append([0.0, 1.0])\n",
    "                Nmid_utterances.append(1)\n",
    "            \n",
    "            if meta[target2]==0:\n",
    "                data_y2.append([1.0, 0.0])\n",
    "            elif meta[target2] == 1:\n",
    "                data_y2.append([0.0, 1.0])\n",
    "\n",
    "    if config.n_classes==3:\n",
    "        for L in utterances_withIDs:\n",
    "            ID = L[1].strip()\n",
    "            meta = meta_data[ID]\n",
    "            if meta[target1]==0:\n",
    "                data_y1.append([1.0, 0.0, 0.0]) \n",
    "                Nlow_utterances.append(1)\n",
    "            elif meta[target1] == 1:\n",
    "                data_y1.append([0.0, 1.0, 0.0])\n",
    "                Nmid_utterances.append(1)\n",
    "            elif meta[target1] == 2:\n",
    "                data_y1.append([0.0, 0.0, 1.0]) #for binary depression categorization, this else should be ignored by default\n",
    "                Nhigh_utterances.append(1)\n",
    "            \n",
    "            if meta[target2]==0:\n",
    "                data_y2.append([1.0, 0.0, 0.0]) \n",
    "            elif meta[target2] == 1:\n",
    "                data_y2.append([0.0, 1.0, 0.0])\n",
    "            elif meta[target2] == 2:\n",
    "                data_y2.append([0.0, 0.0, 1.0]) #for binary depression categorization, this else should be ignored by default\n",
    "    \n",
    "    if config.n_classes==2:\n",
    "        for L in utterances_validation:\n",
    "            ID = L[1].strip()\n",
    "            meta = meta_data[ID]\n",
    "            if meta[target1]==0:\n",
    "                validation_y1.append([1.0, 0.0]) #need labels in this shape for the RNN\n",
    "            elif meta[target1] == 1:\n",
    "                validation_y1.append([0.0, 1.0])\n",
    "            \n",
    "            if meta[target2]==0:\n",
    "                validation_y2.append([1.0, 0.0]) #need labels in this shape for the RNN\n",
    "            elif meta[target2] == 1:\n",
    "                validation_y2.append([0.0, 1.0])\n",
    "        \n",
    "    if config.n_classes==3:\n",
    "        for L in utterances_validation:\n",
    "            ID = L[1].strip()\n",
    "            meta = meta_data[ID]\n",
    "            if meta[target1]==0:\n",
    "                validation_y1.append([1.0, 0.0, 0.0]) #need labels in this shape for the RNN\n",
    "            elif meta[target1] == 1:\n",
    "                validation_y1.append([0.0, 1.0, 0.0])\n",
    "            elif meta[target1] == 2:\n",
    "                validation_y1.append([0.0, 0.0, 1.0]) \n",
    "    \n",
    "            if meta[target2]==0:\n",
    "                validation_y2.append([1.0, 0.0, 0.0]) #need labels in this shape for the RNN\n",
    "            elif meta[target2] == 1:\n",
    "                validation_y2.append([0.0, 1.0, 0.0])\n",
    "            elif meta[target2] == 2:\n",
    "                validation_y.append([0.0, 0.0, 1.0]) \n",
    "    #change each sentence to a sequence of numbers (each number maps to a word), and if the sequence is less than the max_len, pads on the back of this seuqence withs 0s to reach the max sequence length. \n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(config.seq_max_len,min_frequency=config.min_word_frequency)\n",
    "    data_x = np.array(list(vocab_processor.fit_transform(sentences))) #words that don't occur frequently enough are replaced with 0 in data_x. Usually, this (could) be an issue since padding uses 0s as well, but in this code it shouldn't be an issue since we explicitly feed the sequence length (see: https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html). If still concerned, instead, could use another symbol like NA or could remove these words entirely?\n",
    "\n",
    "    #process validation sentences data in way corersponding to dictionary used to process training sentence data\n",
    "    validation_x= np.array(list(vocab_processor.fit_transform(sentences_validation))) #words that don't occur frequently enough are replaced with 0 in data_x. Usually, this (could) be an issue since padding uses 0s as well, but in this code it shouldn't be an issue since we explicitly feed the sequence length (see: https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html). If still concerned, instead, could use another symbol like NA or could remove these words entirely?\n",
    "\n",
    "    ##Get vocab size, needed to specify embedding dimensions in the RNN\n",
    "    vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "    vocab_size= len(vocab_dict)\n",
    "    \n",
    "    #need to reshape utterances_validation, from a list of lists, to a list of strings. each string has 'utterance, ID, line number in transcript, quarter of transcript'\n",
    "    reshaped_utterances_validation=[]  \n",
    "    for i in range(0,len(utterances_validation)):\n",
    "        flatter= \", \".join(utterances_validation[i])\n",
    "        reshaped_utterances_validation.append(flatter)\n",
    "    \n",
    "    '''\n",
    "    #need to reshape utterances_training, from a list of lists, to a list of strings. each string has 'utterance, ID, line number in transcript, quarter of transcript'\n",
    "    reshaped_utterances_training=[]  \n",
    "    for i in range(0,len(utterances_withIDs)):\n",
    "        flatter= \", \".join(utterances_withIDs[i])\n",
    "        reshaped_utterances_training.append(flatter)\n",
    "    '''\n",
    "    \n",
    "    return data_x, data_y1, data_y2, sequence_length, vocab_size, IDs_training, len(Nlow_utterances), len(Nmid_utterances), len(Nhigh_utterances), validation_x, validation_y1, validation_y2, sequence_length_validation, reshaped_utterances_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run this function to get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this needs to be before creating RNN object, since the vocab size is needed to specify the dimensions of the embedding in the RNN \n",
    "dcaps_data, dcaps_label1, dcaps_label2, dcaps_sequence_lengths, vocab_size, dcaps_IDs, N_low, N_mid, N_high, validation_x, validation_y1, validation_y2, sequence_length_validation, utterances_validation = load_data(target1=3, target2=5) \n",
    "\n",
    "check_numbers= (dcaps_data, dcaps_label1, dcaps_label2, dcaps_sequence_lengths, dcaps_IDs, validation_x, validation_y1, validation_y2, utterances_validation, sequence_length_validation)\n",
    "print('Check numbers:')\n",
    "for i in check_numbers:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(N_low, N_mid, N_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#plot out training data sequence lengths\n",
    "plt.hist(dcaps_sequence_lengths, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Distribution of truncated training sequence lengths')\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_debug_sentences(val_predictions1, valBatch_y1, iteration, val_probs1, valBatch_sentences):\n",
    "    os.makedirs('sentence_validation' + str(iteration))\n",
    "    if config.n_classes==2:\n",
    "        with open('sentence_validation' + str(iteration) + '/conf_11.log', 'w') as f11, open('sentence_validation' + str(iteration) + '/conf_12.log', 'w') as f12, open('sentence_validation' + str(iteration) + '/conf_21.log', 'w') as f21, open('sentence_validation' + str(iteration) + '/conf_22.log', 'w') as f22:\n",
    "            files = []\n",
    "            files.append(f11)\n",
    "            files.append(f12)\n",
    "            files.append(f21)\n",
    "            files.append(f22)\n",
    "\n",
    "            for i, val in enumerate(val_predictions1):\n",
    "                file_idx = config.n_classes*val + np.argmax(valBatch_y1[i])\n",
    "                files[file_idx].write(valBatch_sentences[i]+ ',' + str(val_prob_predictions1[i][0]) + ',' + str(val_prob_predictions1[i][1]) + '\\n') \n",
    "           \n",
    "    if config.n_classes==3:\n",
    "        with open('sentence_validation' + str(iteration) + '/conf_11.log', 'w') as f11, open('sentence_validation' + str(iteration) + '/conf_12.log', 'w') as f12, open('sentence_validation' + str(iteration) + '/conf_13.log', 'w') as f13, open('sentence_validation' + str(iteration) + '/conf_21.log', 'w') as f21, open('sentence_validation' + str(iteration) + '/conf_22.log', 'w') as f22, open('sentence_validation' + str(iteration) + '/conf_23.log', 'w') as f23, open('sentence_validation' + str(iteration) + '/conf_31.log', 'w') as f31, open('sentence_validation' + str(iteration) + '/conf_32.log', 'w') as f32, open('sentence_validation' + str(iteration) + '/conf_33.log', 'w') as f33:\n",
    "            files = []\n",
    "            files.append(f11)\n",
    "            files.append(f12)\n",
    "            files.append(f13)\n",
    "            files.append(f21)\n",
    "            files.append(f22)\n",
    "            files.append(f23)\n",
    "            files.append(f31)\n",
    "            files.append(f32)\n",
    "            files.append(f33)\n",
    "        \n",
    "            for i, val in enumerate(val_predictions1):\n",
    "                file_idx = config.n_classes*val + np.argmax(valBatch_y1[i])\n",
    "                files[file_idx].write(valBatch_sentences[i]+ ',' + str(val_prob_predictions1[i][0]) + ',' + str(val_prob_predictions1[i][1]) + ',' + str(val_prob_predictions1[i][2]) + '\\n') \n",
    "    \n",
    "    for thefile in files:\n",
    "        thefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_debug_sentences_training(all_training_predictions1, all_training_y1, iteration, all_training_prob_predictions1, all_training_IDs):\n",
    "    os.makedirs('sentence_training' + str(iteration))\n",
    "    if config.n_classes==2:\n",
    "        with open('sentence_training' + str(iteration) + '/conf_11.log', 'w') as f11, open('sentence_training' + str(iteration) + '/conf_12.log', 'w') as f12, open('sentence_training' + str(iteration) + '/conf_21.log', 'w') as f21, open('sentence_training' + str(iteration) + '/conf_22.log', 'w') as f22:\n",
    "            files = []\n",
    "            files.append(f11)\n",
    "            files.append(f12)\n",
    "            files.append(f21)\n",
    "            files.append(f22)\n",
    "\n",
    "            for i, val in enumerate(all_training_predictions1):\n",
    "                file_idx = config.n_classes*val + np.argmax(all_training_y1[i])\n",
    "                files[file_idx].write(str(all_training_IDs[i]) + ',' + str(all_training_prob_predictions1[i][0]) + ',' + str(all_training_prob_predictions1[i][1]) + '\\n') \n",
    "    \n",
    "    if config.n_classes==3:\n",
    "        with open('sentence_training' + str(iteration) + '/conf_11.log', 'w') as f11, open('sentence_training' + str(iteration) + '/conf_12.log', 'w') as f12, open('sentence_training' + str(iteration) + '/conf_13.log', 'w') as f13, open('sentence_validation_training' + str(iteration) + '/conf_21.log', 'w') as f21, open('sentence_training' + str(iteration) + '/conf_22.log', 'w') as f22, open('sentence_validation_training' + str(iteration) + '/conf_23.log', 'w') as f23, open('sentence_training' + str(iteration) + '/conf_31.log', 'w') as f31, open('sentence_training' + str(iteration) + '/conf_32.log', 'w') as f32, open('sentence_training' + str(iteration) + '/conf_33.log', 'w') as f33:\n",
    "            files = []\n",
    "            files.append(f11)\n",
    "            files.append(f12)\n",
    "            files.append(f13)\n",
    "            files.append(f21)\n",
    "            files.append(f22)\n",
    "            files.append(f23)\n",
    "            files.append(f31)\n",
    "            files.append(f32)\n",
    "            files.append(f33)\n",
    "        \n",
    "            for i, val in enumerate(all_training_predictions1):\n",
    "                file_idx = config.n_classes*val + np.argmax(all_training_y1[i])\n",
    "                files[file_idx].write(str(all_training_prob_predictions1[i][0]) + ',' + str(all_training_prob_predictions1[i][1]) + ',' + str(all_training_predictions1[i][2]) + '\\n') \n",
    "    \n",
    "    for thefile in files:\n",
    "        thefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write_debug_sentences_training(all_training_predictions1 , all_training_y1, step*batch_siz, all_training_prob_predictions1, all_training_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that data is loaded in, begin building RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights1, biases1,weights2, biases2, vocab_size, keep_prob):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    #need to input dat in: batch_size, n_steps\n",
    "    with tf.device(\"/cpu:0\"): #http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "        embedding_mat = tf.Variable(tf.random_uniform([vocab_size, config.embedding_size], -1.0, 1.0)) #this should be a matrix vocab size by embedding size\n",
    "        embedding_output = tf.nn.embedding_lookup(embedding_mat, x)\n",
    "    #The result of the embedding operation is a 3-dimensional tensor of shape [None, sequence_length, embedding_size].\n",
    "   \n",
    "    # Define a lstm cell with tensorflow\n",
    "    if config.num_layers == 1:\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(config.n_hidden)\n",
    "        if config.keep_prob < 1:\n",
    "            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob, input_keep_prob=keep_prob) #might also add input_keep_prob\n",
    "        #init_state = lstm_cell.zero_state(config.batch_size, tf.float32)    \n",
    "    else:    \n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(config.n_hidden, reuse=tf.get_variable_scope().reuse)\n",
    "        lstm_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(config.num_layers)], state_is_tuple = True)\n",
    "        if config.keep_prob <1:\n",
    "            def lstm_cell():\n",
    "                return tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(config.n_hidden), output_keep_prob=keep_prob, input_keep_prob=keep_prob)\n",
    "            lstm_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(config.num_layers)], state_is_tuple = True)\n",
    "\n",
    "        #lstm_cell = tf.contrib.rnn.BasicLSTMCell(config.n_hidden, reuse=tf.get_variable_scope().reuse)\n",
    "        #init_state = lstm_cell.zero_state(config.batch_size, tf.float32)            \n",
    "                \n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic calculation.\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, embedding_output, dtype=tf.float32, sequence_length=seqlen)\n",
    "    \n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e, if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "\n",
    "    output = tf.transpose(outputs, [1, 0, 2])\n",
    "    last= tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "    \n",
    "    return tf.matmul(last, weights1['out']) + biases1['out'], tf.matmul(last, weights2['out']) + biases2['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred1, pred2 = dynamicRNN(x, seqlen, weights1, biases1, weights2, biases2, vocab_size, keep_prob) #returns a tensor [#, #, #] with LOGITS for each class, only become probabilities when fed into softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob1 = tf.nn.softmax(pred1) #get probabilities as a variable, so they can be added in and analyzed later\n",
    "predictions1 = tf.cast(tf.argmax(pred1,1), tf.int64) #a_note, returns the index with the largest value across axis of a tensor, i.e., the predicted class as 1/2/3\n",
    "labels1 = tf.cast(tf.argmax(y1,1), tf.int64) #a_note, returns the index with the largest value across axis of a tensor, i.e., the expected class as 1/2/3\n",
    "conf_mat1 = tf.confusion_matrix(labels= predictions1, predictions= labels1) #confusion matrix, where we WANT each row is a prediction, each column a true label\n",
    "\n",
    "prob2 = tf.nn.softmax(pred2) #get probabilities as a variable, so they can be added in and analyzed later\n",
    "predictions2 = tf.cast(tf.argmax(pred2,1), tf.int64) #a_note, returns the index with the largest value across axis of a tensor, i.e., the predicted class as 1/2/3\n",
    "labels2 = tf.cast(tf.argmax(y2,1), tf.int64) #a_note, returns the index with the largest value across axis of a tensor, i.e., the expected class as 1/2/3\n",
    "conf_mat2 = tf.confusion_matrix(labels= predictions2, predictions= labels2 ) #confusion matrix, where we WANT each row is a prediction, each column a true label\n",
    "\n",
    "###################################\n",
    "\n",
    "# Define loss and optimizer\n",
    "\n",
    "\n",
    "#FOR COST-SENSITIVE LEARNING:\n",
    "#####use Cost-sensitive learning if proportion of classes in data is very unbalanced \n",
    "#(under samping the majority class is another approach). \n",
    "#Create and add weights to labels for cost-sensitive learning, where the cost is magnified for certain observations, \n",
    "#such as those which occur in/frequently. Example followed: https://github.com/jakeret/tf_unet/blob/master/tf_unet/unet.py#L191\n",
    "\n",
    "\n",
    "if config.n_classes==2:\n",
    "    if config.balance_classes == True:\n",
    "        prop_low = (N_low+N_mid)/N_low #calculate weights, which are the proportion of each class\n",
    "        prop_mid = (N_low+N_mid)/N_mid \n",
    "        classes_weights = tf.constant([prop_low, prop_mid],  dtype=np.float32) #weighted contributions of each class, so that we can penalize extra when a sentences is mistakenly put in the majority class. otherwise the most 'accurate' strategy in unbalanced data is to predict everthing as the majority class \n",
    "    else:\n",
    "        classes_weights = tf.constant([1.0, 1.4],  dtype=np.float32) #weighted contributions of each class, so that we can penalize extra when a sentences is mistakenly put in the majority class. otherwise the most 'accurate' strategy in unbalanced data is to predict everthing as the majority class \n",
    "\n",
    "if config.n_classes==3: \n",
    "    if config.balance_classes == True:\n",
    "        prop_low = (N_low+N_mid+N_high)/N_low #calculate weights, which are the proportion of each class\n",
    "        prop_mid = (N_low+N_mid+N_high)/N_mid \n",
    "        prop_high = (N_low+N_mid+N_high)/N_high #consider increaseing penalty by muliptlying N_high*.7 or another value\n",
    "        classes_weights = tf.constant([prop_low, prop_mid, prop_high],  dtype=np.float32) #weighted contributions of each class, so that we can penalize extra when a sentences is mistakenly put in the majority class. otherwise the most 'accurate' strategy in unbalanced data is to predict everthing as the majority class \n",
    "    else:\n",
    "        classes_weights = tf.constant([1.2, 1.3, 1.5],  dtype=np.float32) #weighted contributions of each class, so that we can penalize extra when a sentences is mistakenly put in the majority class. otherwise the most 'accurate' strategy in unbalanced data is to predict everthing as the majority class \n",
    "    \n",
    "#weighted_logits = tf.multiply(pred, classes_weights) #could use weighted_logits instead of weighted preds in cost function\n",
    "\n",
    "#just weight the predictions for depression status, which is what we care about most:\n",
    "flat_pred = tf.reshape(pred1, [-1, config.n_classes])\n",
    "flat_labels = tf.reshape(y1, [-1, config.n_classes])\n",
    "weight_map1 = tf.multiply(flat_labels, classes_weights )\n",
    "weight_map = tf.reduce_sum(weight_map1, axis=1) #sum weighted predictions\n",
    "\n",
    "#weighted cost, with downweighted cost for pred2 since pred1 (dep status) is what we care about most:\n",
    "loss_map= tf.nn.softmax_cross_entropy_with_logits(labels=flat_labels, logits=flat_pred) #shape [1, batch_size] #get loss\n",
    "weighted_cost1= tf.multiply(loss_map, weight_map) #weight the loss by the summed, weighted predictions (why? multiplies loss from rarer events by larger factor, so they won't just be put in the most common class. )\n",
    "cost1 = tf.reduce_mean(weighted_cost1) #shape 1; returns the mean of weighted losses\n",
    "\n",
    "reg1= tf.nn.l2_loss(weights1['out'])\n",
    "reg2= tf.nn.l2_loss(weights2['out'])\n",
    "\n",
    "cost = cost1 + .6*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y2, logits=pred2)) + config.L2_penalty*reg1 + config.L2_penalty*reg2 # this loss in middle is the addded loss plus L2 penalty term. why? shouldnt the mean cost be added to the regualrized mean weights?\n",
    "\n",
    "\n",
    "\n",
    "#FOR NON-COST SENSITIVE LEARNING:\n",
    "'''\n",
    "######L2 REGULARIZATION. But if config.L2_penalty is 0 this reduces to no L2 regularization: Not 100% confident in code yet, so keep at 0. \n",
    "reg1= tf.nn.l2_loss(weights1['out'])\n",
    "reg2= tf.nn.l2_loss(weights2['out'])\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y1, logits=pred1)) + .6*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y2, logits=pred2)) + config.L2_penalty*reg1 + config.L2_penalty*reg2 # this loss in middle is the addded loss plus L2 penalty term. why? shouldnt the mean cost be added to the regualrized mean weights?\n",
    "\n",
    "'''\n",
    "\n",
    "#OPTIMIZER\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=config.learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "#EVALUATE MODEL\n",
    "#pred = tf.Print(pred, [tf.argmax(pred, 1), tf.argmax(y,1)], message='crisp_prediction', summarize=100)\n",
    "correct_pred1 = tf.equal(tf.argmax(pred1,1), tf.argmax(y1,1))\n",
    "accuracy1 = tf.reduce_mean(tf.cast(correct_pred1, tf.float32))\n",
    "\n",
    "correct_pred2 = tf.equal(tf.argmax(pred2,1), tf.argmax(y1,1))\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_pred2, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init =  tf.global_variables_initializer() #tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver() #for saving models at each step where validation accuracy is assessed\n",
    "    step = 1\n",
    "    moving_avg_loss = 0.0\n",
    "    moving_avg_acc = 0.0\n",
    "    discount = 0.01\n",
    "    batch_siz = config.batch_size \n",
    "\n",
    "    config.printConfiguration()\n",
    "    print(len(dcaps_data), len(dcaps_label1), len(dcaps_sequence_lengths))\n",
    "    valBatch_x, valBatch_y1,valBatch_y2, valBatch_seqlen, valBatch_sentences = random_batch(validation_x, validation_y1,validation_y2, sequence_length_validation, part_size=7500, sentences=utterances_validation) #sentences_validation\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_siz < config.training_iters:\n",
    "        # batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "   \n",
    "        batch_x, batch_y1,batch_y2, batch_seqlen, __ = random_batch(dcaps_data, dcaps_label1, dcaps_label2,dcaps_sequence_lengths, part_size=batch_siz)\n",
    "\n",
    "        #Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y1: batch_y1, y2: batch_y2, seqlen: batch_seqlen, keep_prob:config.keep_prob})\n",
    "        if step % config.display_step == 0:\n",
    "            #Calculate batch accuracy\n",
    "            acc = sess.run(accuracy1, feed_dict={x: batch_x, y1: batch_y1,y2: batch_y2,\n",
    "                                               seqlen: batch_seqlen, keep_prob:config.keep_prob})\n",
    "            #Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y1: batch_y1,y2: batch_y2,\n",
    "                                            seqlen: batch_seqlen, keep_prob:config.keep_prob})\n",
    "\n",
    "            if step == 1:\n",
    "                moving_avg_loss = loss\n",
    "                moving_avg_acc = acc\n",
    "            else:\n",
    "                moving_avg_loss = (1.0-discount) * moving_avg_loss + discount * loss\n",
    "                moving_avg_acc = (1.0-discount) * moving_avg_acc + discount * acc\n",
    "            print(\"Iter \" + str(step*batch_siz) + \", Minibatch Loss= \" + \\\n",
    "                \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                \"{:.5f}\".format(acc)  + \", Moving Average Loss= \" + \\\n",
    "                \"{:.6f}\".format(moving_avg_loss) + \", Moving Average Acc= \" + \\\n",
    "                \"{:.5f}\".format(moving_avg_acc))\n",
    "        if step % config.val_step == 0:\n",
    "\n",
    "            print (\"Testing Accuracy:\", \\\n",
    "                sess.run(accuracy1, feed_dict={x: valBatch_x, y1: valBatch_y1, y2: valBatch_y2,\n",
    "                                              seqlen: valBatch_seqlen, keep_prob:1}))\n",
    "            print (\"Testing Accuracy2:\", \\\n",
    "                sess.run(accuracy2, feed_dict={x: valBatch_x, y1: valBatch_y1, y2: valBatch_y2,\n",
    "                                              seqlen: valBatch_seqlen, keep_prob:1}))\n",
    "            print (\"Test Confmatrix: \\n\", \\\n",
    "            sess.run(conf_mat1, feed_dict={x: valBatch_x, y1: valBatch_y1, y2: valBatch_y2,  seqlen: valBatch_seqlen, keep_prob:1}))\n",
    "            val_predictions1 = sess.run(predictions1, feed_dict={x: valBatch_x, y1: valBatch_y1,  y2: valBatch_y2,seqlen: valBatch_seqlen, keep_prob:1}) #get predicted classes for validation batch\n",
    "            val_prob_predictions1 = sess.run(prob1, feed_dict={x: valBatch_x, y1: valBatch_y1,  y2: valBatch_y2,seqlen: valBatch_seqlen, keep_prob:1}) #get predicted probabilities for validation batch\n",
    "            \n",
    "            if config.debug_sentences: \n",
    "                write_debug_sentences(val_predictions1, valBatch_y1, step*batch_siz, val_prob_predictions1, valBatch_sentences)\n",
    "                \n",
    "                all_training_x, all_training_y1,all_training_y2, all_training_seqlen, all_training_IDs = random_batch(dcaps_data, dcaps_label1, dcaps_label2,dcaps_sequence_lengths, part_size=len(dcaps_data), sentences= dcaps_IDs) #just write ids not whole sentences, to save computational time here\n",
    "                all_training_predictions1 = sess.run(predictions1, feed_dict={x: all_training_x, y1: all_training_y1,  y2: all_training_y2,seqlen: all_training_seqlen, keep_prob:1}) #get predicted classes for all training\n",
    "                all_training_prob_predictions1 = sess.run(prob1, feed_dict={x: all_training_x, y1: all_training_y1,  y2: all_training_y2,seqlen: all_training_seqlen, keep_prob:1})  #get predicted probs for all training \n",
    "                write_debug_sentences_training(all_training_predictions1 , all_training_y1, step*batch_siz, all_training_prob_predictions1, all_training_IDs)\n",
    "            #save a model\n",
    "            #saver.save(sess, 'my_test_model',global_step=step)\n",
    "        step += 1\n",
    "    print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
