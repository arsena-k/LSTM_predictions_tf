{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An LSTM Neural Net to predict depression, or depression symptoms based on the PHQ-8 from transcribed verbal data \n",
    "\n",
    "### Specifically, data are utterances which includes the words uttered (learned as embeddings) and linguistic features utterance from LIWC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this code is exploratory, may contain errors!\n",
    "* python 2 with Tensorflow\n",
    "* prediction options include: depression level (continuous, low/med/high, or binary), level for each depression symptom based on PHQ-8 (low/med/high, or binary). Need to change the cost function in the code depending on the outcome chosen. \n",
    "* data is unbalanced, so options in code for 1) undersampling the majority class (lower levels of depression) and 2) cost-senstive learning\n",
    "* various options for drop out, L1/L2 regaularization\n",
    "\n",
    "Unique features of this model compared to other frameworks explored:\n",
    "* includes embedding layer prior to LSTM layer. Here, The emebddings are learned along the way not pre-trained. \n",
    "* concatenates the utterances (turned into word-embeddings) with and linguistics features of the utterances (from LIWC)\n",
    "* includes dense (fully connected) output layer\n",
    "* sequences of various lengths may be used. In this code, there is no option yet written for a sliding window of words (see the other LSTM code in this repo for examples).\n",
    "* debug_sentences allows us to write the sentences in each part of the confusion matrix to a file for later examination of the erors\n",
    "\n",
    "\n",
    "Alina notes to self:\n",
    "* data corresponding to IDs without meta-information and data corresponding to missing target information are excluded in Load_Data()\n",
    "* for variable coding see codebook excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Helpful tutorials:\n",
    "* https://github.com/nfmcclure/tensorflow_cookbook/blob/master/09_Recurrent_Neural_Networks/02_Implementing_RNN_for_Spam_Prediction/02_implementing_rnn.py\n",
    "* Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf; Author: Aymeric Damien; Project: https://github.com/aymericdamien/TensorFlow-Examples/    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from decimal import *\n",
    "\n",
    "cwd= os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==========\n",
    "#   MODEL\n",
    "# Some LSTM code and code structure reused from M. Morales\n",
    "\n",
    "# ==========\n",
    "class BaseConfiguration:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.0001 #also try .001, i think accuracy and confusion matrix could be more stable with lower learning rate\n",
    "        self.training_iters = 30000000  #was 50,000,000. Consider reducing further if you notice that the testing accuracy becomes stable (or starts dropping) earlier than this number of iterations, to prevent overfitting. \n",
    "        self.batch_size = 50\n",
    "        self.display_step = 1\n",
    "        self.val_step = 50\n",
    "        self.L2_penalty= 0.0 #beta term on L2 weight penalty for L2 normalization, if set to 0.0 for no L2 normalization. I think confusion matrix becomes unbalanced (but accuracy higher) with L2 noamlization\n",
    "        # Network Parameters\n",
    "        self.seq_max_len = 30 # Sequence max length. \n",
    "        self.seq_min_len = 7\n",
    "        self.embedding_size = 20 #so far best was 10 emebdding, 15 hidden\n",
    "        self.n_hidden = 15 # hidden layer num of features, per layer\n",
    "        self.n_classes = 1 # linear sequence or not, this code is meant for linear sequence only!\n",
    "        self.num_layers = 1 #to add more layers: https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "        self.keep_prob = .9 #for no dropout, use 1. This only performs dropout on the LSTM layer, for outputs. If config.n_layers>1, dropout will work on every layer, i.e., if set dropout to .9 and 2 layers, total dropout is ,.9*.9\n",
    "        self.debug_sentences = True #if True, sentences corresponding to each cell in the confusion matrix are written in folder \"sentence_validation\"\n",
    "        self.min_word_frequency=15 #wont work well below 15\n",
    "        self.validation_IDs_proportion=.1 #proportion of IDs (among IDs that exist in meta data, and have the specified target value) that are held out as \"validation\" data\n",
    "        self.balance_classes=True #use for balancing classification with undersampling. If True, will only sample the number of the minority category utterances to be same as high utterances (for training data)\n",
    "        self.linguistic_feats= 56 #this is what is in utterance with LIWC features file as is, includes 55 ling feats and 1 feat for #words\n",
    "    def printConfiguration(self):\n",
    "        # print configuration\n",
    "        print ('---------- Configuration: ----------')\n",
    "        print ('learning_rate', self.learning_rate)\n",
    "        print ('training_iters', self.training_iters)\n",
    "        print ('batch_size', self.batch_size) \n",
    "        print ('L2_penalty', self.L2_penalty) \n",
    "\n",
    "        # Network Parameters\n",
    "        print ('seq_max_len', self.seq_max_len )# Sequence max length\n",
    "        print ('seq_min_len', self.seq_min_len)\n",
    "        print ('embedding_size', self.embedding_size)\n",
    "        print ('n_hidden', self.n_hidden) # hidden layer num of features\n",
    "        print ('n_classes', self.n_classes) # linear sequence or not\n",
    "        print ('num_layers', self.num_layers)\n",
    "        print ('keep_prob (dropout = 1-keep_prob)', self.keep_prob)\n",
    "        print ('------------------------------------')\n",
    "\t#print 'n_hidden: ', self.n_hidden, 'learning rate: ', self.learning_rate, ' batch_size: ', self.batch_size, ' max_len: ', self.seq_max_len, ' min_len: ', self.seq_min_len\n",
    "\n",
    "# Parameters\n",
    "# configuration\n",
    "config = BaseConfiguration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #to reset tensorflow graph if you've already made one graph, once re-set need to run through to get variable agani. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Tensorflow Graph Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "#x = tf.placeholder(tf.float32, [None, config.seq_max_len]) #this placeholder for x used if pre-trained embeddings are used\n",
    "y = tf.placeholder(tf.float32, [None, config.n_classes]) \n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, config.seq_max_len]) #this placeholder for x used if training embedding layer\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "ling_features= tf.placeholder(tf.float32, [None, config.linguistic_feats])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([config.n_hidden+config.linguistic_feats, config.n_classes])) #note weights are initialized as normal random variable\n",
    "}\n",
    "\n",
    "\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([config.n_classes]))  #note biases are initialized as normal random variable\n",
    "}\n",
    "\n",
    "#need to add another set of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(x, y, sequence_length, LIWC, part_size=0, sentences=None):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    batch_sequence_length = []\n",
    "    batch_sentences = []\n",
    "    batch_LIWC=[]\n",
    "    indexes = range(len(x))\n",
    "    random.shuffle(indexes)\n",
    "    #print(indexes[0:batch_size-1])\n",
    "    for index in indexes[0:part_size-1]:\n",
    "        batch_x.append(x[index])\n",
    "        batch_y.append(y[index])\n",
    "        batch_sequence_length.append(sequence_length[index])\n",
    "        batch_LIWC.append(LIWC[index])\n",
    "        if sentences is not None:\n",
    "            batch_sentences.append(sentences[index])\n",
    "    return batch_x, batch_y, batch_sequence_length, batch_LIWC, batch_sentences\n",
    "\n",
    "def last_relevant(output, length):\n",
    "    config.batch_size = tf.shape(output)[0]\n",
    "    max_length = int(output.get_shape()[1])\n",
    "    output_size = int(output.get_shape()[2])\n",
    "    index = tf.range(0, config.batch_size) * max_length + (length - 1)\n",
    "    flat = tf.reshape(output, [-1, output_size])\n",
    "    relevant = tf.gather(flat, index)\n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_meta_data(path):\n",
    "    \"\"\"Imports labels as a dictionary- must designate below which label you want\"\"\"\n",
    "    labels = {}\n",
    "    # load the CSV file as a numpy matrix\n",
    "    with open (path,'r') as csv:\n",
    "        dataset = csv.readlines()\n",
    "    for row in dataset[1:]:\n",
    "        ID, depression, dep_binary2, depression_value, depression_level_official, Morethan7PHQsympt_Available,PHQ_9NoInterest,PHQ_9Depressed,PHQ_9Sleep,PHQ_9Tired,PHQ_9Appetite,PHQ_9Failure,PHQ_9Concentrating,PHQ_9Moving,PHQ_9NoInterest_level,PHQ_9Depressed_level,PHQ_9Sleep_level,PHQ_9Tired_level,PHQ_9Appetite_level,PHQ_9Failure_level,PHQ_9Concentrating_level,PHQ_9Moving_level  = row.strip().split(',')\n",
    "        #map depression strings to int values\n",
    "        try:\n",
    "            depression_value= int(depression_value.strip())/32 #rescale to be between 0 and 1, max value on PHQ is 8*4=32\n",
    "        except:\n",
    "            depression_value ='NA'\n",
    "        depression_level_official = depression_level_official.strip()\n",
    "        PHQ_9NoInterest_level = PHQ_9NoInterest_level.strip()\n",
    "        PHQ_9Depressed_level = PHQ_9Depressed_level.strip()\n",
    "        PHQ_9Sleep_level = PHQ_9Sleep_level.strip()\n",
    "        PHQ_9Tired_level = PHQ_9Tired_level.strip()\n",
    "        PHQ_9Appetite_level = PHQ_9Appetite_level.strip()\n",
    "        PHQ_9Failure_level = PHQ_9Failure_level.strip()\n",
    "        PHQ_9Concentrating_level = PHQ_9Concentrating_level.strip()\n",
    "        PHQ_9Moving_level = PHQ_9Moving_level.strip()  \n",
    "        \n",
    "        labels[ID.strip()] = [depression, dep_binary2, depression_value, depression_level_official,PHQ_9NoInterest_level,PHQ_9Depressed_level,PHQ_9Sleep_level,PHQ_9Tired_level,PHQ_9Appetite_level,PHQ_9Failure_level,PHQ_9Concentrating_level,PHQ_9Moving_level]\n",
    "    return labels\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#for testing:\n",
    "#meta_data = import_meta_data('Meta_Data_with_Symptoms.csv')\n",
    "#meta_data['668'] #311 should have dep score, 668 should have NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for testing:\n",
    "f = open(cwd+ '/LIWC/re_cleaned_utterances_withLIWC.txt','r')\n",
    "#lines =  [line.strip().split(',') for line in f.readlines()]\n",
    "#lines2 =  [line.split(',') for line in f.readlines()]\n",
    "lines =  [line.strip().split(',') for line in f.readlines()]\n",
    "for line in lines:\n",
    "    del line[-1] #because comma at end of last element on a line, need to delete this lingering, empty element\n",
    "f.close()\n",
    "\n",
    "\n",
    "liwc_nums= map(float, lines[10][4:len(lines[10])])\n",
    "liwc_nums= map(Decimal, liwc_nums[4:len(liwc_nums)])     \n",
    "len(liwc_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relevant= []\n",
    "\n",
    "for in lines:\n",
    "    if liwc_nums:\n",
    "        relevant.append(1) #relevant\n",
    "    else liwc_nums:\n",
    "        relevant.append(0) #not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_meta=[] #this is just for inspecting IDs that aren't in the meta-data file, and so not used in RNN code\n",
    "IDs_NAs_inMeta=[] #this is just for inspecting IDs that are in the meta-data file but don't have target values, so not used in RNN code\n",
    "\n",
    "def load_data(target=None, data_file_path= cwd+ '/LIWC/re_cleaned_utterances_withLIWC.txt', meta_file_path='Meta_Data_with_Symptoms.csv'):\n",
    "\n",
    "    if target is None:\n",
    "        print(\"Target not specified: possible target classes: depression_binary=0, dep_binary2=1, depression_value=2, depression_level_official=3, PHQ_9NoInterest_level=4,PHQ_9Depressed_level=5,PHQ_9Sleep_level=6,PHQ_9Tired_level=7,PHQ_9Appetite_level=8,PHQ_9Failure_level=9,PHQ_9Concentrating_level=10,PHQ_9Moving_level=11\")\n",
    "        raise\n",
    "    #get labels\n",
    "    meta_data = import_meta_data(meta_file_path)\n",
    "    \n",
    "    #load utterances data\n",
    "    f = open(data_file_path,'r')\n",
    "    lines =  [line.strip().split(',') for line in f.readlines()]\n",
    "    for line in lines:\n",
    "        del line[-1] #because comma at end of last element on a line, need to delete this lingerin and empty element\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    #training data variables\n",
    "    utterances_withIDs = [] #utterances of training data with meta data (ID of speaker, line in transcript, and quarter of transcript)\n",
    "    data_y = [] #this will be an array with an element for each sentence in the training data, with the depression label, in the format [X,X,X] \n",
    "    sequence_length = [] #vector of sequence lengths for each utterances in the training data\n",
    "    sentences = [] #just the utterances of training data, no meta data included \n",
    "    IDs = [] #training IDs\n",
    "    LIWC= [] \n",
    "    #validation data variables\n",
    "    utterances_validation=[]  #utterances of validation data with meta data (ID of speaker, line in transcript, and quarter of transcript)\n",
    "    validation_y =[]  #this will be an array with an element for each sentence in the validation data, with the depression label, in the format [X,X,X] \n",
    "    sequence_length_validation=[] #vector of sequence lengths for each utterances in the training data\n",
    "    sentences_validation=[] #just the utterances of validation data, no meta data included \n",
    "    IDs_validation=[] #list of validation IDs so that validation ID's are excluded from training data, and it is in same order as the utterances, sentences, sequence length, y, etc. \n",
    "    LIWC_validation =[]\n",
    "    \n",
    "    #get validation IDs, and list them in validation_IDs_set\n",
    "    IDs_noNAs_inMeta=[] #list of IDs in sentences that have corresponding meta data and have the specified target value\n",
    "    for L in lines:\n",
    "        ID = L[1].strip() \n",
    "        try:\n",
    "            meta = meta_data[ID]\n",
    "            if meta[target] != 'NA':\n",
    "                IDs_noNAs_inMeta.append(ID)\n",
    "            else:\n",
    "                IDs_NAs_inMeta.append(ID) #just in case want to track who was excluded\n",
    "        except:\n",
    "            missing_meta.append(ID)\n",
    "            continue       \n",
    "    validation_IDs_set= random.sample(set(IDs_noNAs_inMeta), int(round(config.validation_IDs_proportion*len(set(IDs_noNAs_inMeta))))) #get number of IDs for validation set based on the proportion set in the configuration, round the number to nearest integer\n",
    "    \n",
    "    for L in lines:\n",
    "        ID = L[1].strip()    \n",
    "        length = len(L[0].split(\" \")) #each line is for example: ['me there at all can you do it at home', ' 602', '1', 'first quarter'], so the 0th item is the words, this is measuring characters, if want to measure words split here split.(\"\\\\s+\")\n",
    "        if length > config.seq_max_len: #truncate at max sequence length\n",
    "            length=config.seq_max_len\n",
    "        minimum_seqs= config.seq_min_len-1 #such that the minimum length in the next line is inclusive of the minimum number itself (e.g. if minimum sequence length is 6 words then sequences of 6 will be included)\n",
    "        \n",
    "        if length > minimum_seqs: #and L[3]!='first_quarter' #use this last piece if want to exclude section of each persons transcript (sections: last_quarter, first_quarter, middle_half)\n",
    "            try:\n",
    "                meta = meta_data[ID]\n",
    "                if meta[target] != 'NA': #exclude participants without data for the specific target\n",
    "                    if ID not in validation_IDs_set: #IDs in validation set are EXCLUDED from variables below\n",
    "                        sentences.append(L[0]) \n",
    "                        utterances_withIDs.append(L) # can sample among these,below #this is a list of [utterance, id] for each utterance. but only includes those corresponding to indivduals with valid ids\n",
    "                        IDs.append(ID)\n",
    "                        sequence_length.append(length)\n",
    "                        data_y.append([meta[target]])\n",
    "                        liwc_nums= map(float, L[4:len(L)])\n",
    "                        liwc_nums= map(Decimal, liwc_nums[4:len(liwc_nums)])     \n",
    "                        LIWC.append(liwc_nums)\n",
    "                    else:\n",
    "                        sentences_validation.append(L[0]) \n",
    "                        utterances_validation.append(L) \n",
    "                        IDs_validation.append(ID)\n",
    "                        sequence_length_validation.append(length)\n",
    "                        validation_y.append([meta[target]])\n",
    "                        liwc_nums= map(float, L[4:len(L)])\n",
    "                        liwc_nums= map(Decimal, liwc_nums[4:len(liwc_nums)])     \n",
    "                        LIWC_validation.append(liwc_nums)        \n",
    "            except:\n",
    "                #print('invalid ID: ', ID)\n",
    "                continue\n",
    "   \n",
    "    #change each sentence to a sequence of numbers (each number maps to a word), and if the sequence is less than the max_len, pads on the back of this seuqence withs 0s to reach the max sequence length. \n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(config.seq_max_len,min_frequency=config.min_word_frequency)\n",
    "    data_x = np.array(list(vocab_processor.fit_transform(sentences))) #words that don't occur frequently enough are replaced with 0 in data_x. Usually, this (could) be an issue since padding uses 0s as well, but in this code it shouldn't be an issue since we explicitly feed the sequence length (see: https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html). If still concerned, instead, could use another symbol like NA or could remove these words entirely?\n",
    "\n",
    "    #process validation sentences data in way corersponding to dictionary used to process training sentence data\n",
    "    validation_x= np.array(list(vocab_processor.fit_transform(sentences_validation))) #words that don't occur frequently enough are replaced with 0 in data_x. Usually, this (could) be an issue since padding uses 0s as well, but in this code it shouldn't be an issue since we explicitly feed the sequence length (see: https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html). If still concerned, instead, could use another symbol like NA or could remove these words entirely?\n",
    "\n",
    "    ##Get vocab size, needed to specify embedding dimensions in the RNN\n",
    "    vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "    vocab_size= len(vocab_dict)\n",
    "    \n",
    "    #need to reshape utterances_validation, from a list of lists, to a list of strings. each string has 'utterance, ID, line number in transcript, quarter of transcript'\n",
    "    reshaped_utterances_validation=[]  \n",
    "    for i in range(0,len(utterances_validation)):\n",
    "        flatter= \", \".join(utterances_validation[i])\n",
    "        reshaped_utterances_validation.append(flatter)\n",
    "    \n",
    "    return data_x, data_y, sequence_length, IDs, vocab_size, sentences, LIWC, validation_x, validation_y, sequence_length_validation, IDs_validation,sentences_validation, reshaped_utterances_validation, LIWC_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcaps_data, dcaps_label, dcaps_sequence_lengths, dcaps_IDs, vocab_size, dcaps_sentences, LIWC_feats,validation_x, validation_y, sequence_length_validation, IDs_validation,sentences_validation, utterances_validation, LIWC_feats_validation = load_data(target=2) #this needs to be before creating RNN object, since the vocab size is needed to specify the dimensions of the embedding in the RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46209\n",
      "46209\n",
      "46209\n",
      "46209\n",
      "46209\n",
      "5405\n",
      "5405\n",
      "5405\n",
      "5405\n",
      "5405\n"
     ]
    }
   ],
   "source": [
    "check_numbers= (dcaps_data, dcaps_label, dcaps_sequence_lengths, dcaps_IDs, dcaps_sentences, validation_x, validation_y, sentences_validation, utterances_validation, sequence_length_validation)\n",
    "for i in check_numbers:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XuYXVWZ5/Hvj0AEE25CySUXgpqBDghpLIJO02JGQYJo\nvNASRAWEiekRgbHtAS8gKA70iHZDNxrCRQM0Al6wI0a5zAQQgSEJ9yBgDIEkRJJwTVCBwNt/rFWy\nc6iqsyrJrjpV9fs8Tz11zr6s8+599tnvWWvts7YiAjMzs2Y26esAzMysf3DCMDOzIk4YZmZWxAnD\nzMyKOGGYmVkRJwwzMyvihFFA0nRJp26kskZLWiNpSH5+k6TjNkbZubxfSjpqY5XXg9c9U9IqSX/o\n7dfua5IWS3pfjeUfKen6jb2sNSdpjKSQtGkfvPbRkm7t7dftzqBPGPnD/idJqyU9K+k2SdMk/WXf\nRMS0iPhGYVndnjgi4vGIGB4Rr2yE2E+XdHlD+ZMiYuaGlt3DOEYD/wCMi4gdO5n/HklLezOmUnXH\nJukHks7ckDIi4t8j4qCNvay1jr5MTD0x6BNG9sGI2BLYBTgbOBm4eGO/SKsfDBtgNPBURKxY3wIG\n8L7p1mDdbuunImJQ/wGLgfc1TJsAvArsmZ//ADgzP94euBZ4Fnga+DUp8V6W1/kTsAb4X8AYIIBj\ngceBWyrTNs3l3QScBdwJPA/8B/CmPO89wNLO4gUOBl4CXs6vd2+lvOPy402ArwKPASuAS4Gt87yO\nOI7Ksa0CvtLNfto6r78yl/fVXP778ja/muP4QcN6wxrmrwF2Bk4Hfgxcnrf7uOp+7mz787Z/EbgP\neA64Cti8Mn8ycE8u7/fAwXn6McBvgdXAIuCzTWLbBDgll/EUcHXHe5LX+1TeB08BX6GTYygvNzW/\nPy/lsn9e2Y6T83a8CGxaeb3VwIPARyrlHA3cWnkewDTgd6Tj8HxA67HsEODb+b1/FDieyrHZyfac\nDCzLMT4MvLdynK3X/ip4z3cGfkI67h4FTqjMOz2/1qU5pgVAe2X+KOCned2ngH+rzPtMPiaeAa4D\ndulim8ew7ud1a9KXyeV5X5wJDKnue+CcXO6jwKRKWbuSzgGrgRvze3F5nvd4fp2O4/BdBeUdTTqe\nV+d5R9Z+vuzNk3Mr/tH1h/1x4O8bD2rSyX06sFn++9vKB3CdsioH26Wkk9MWnRyAN+UDb8+8zE8q\nB9E6H57G18gfmMsb5t/EawnjM8BC4C3A8PzhuawhtgtzXHuTTl5/1cV+upSUzLbM6z4CHNtVnA3r\ndrYdp5NOph8mnXC2oCxh3Ek6ibyJ9IGfludNICWRA3N5I4Dd87wPAG8FBBwA/BHYp5vYTgTuAEYC\nbwAuAH6Y540jfaDfned9B1hLJ8dQ47HTsB33kE5oW+Rpf8dryepw4AVgpzzvaF6fBK4FtiHV7lby\nWnLsybLTSMlpJLAt6STWacIAdgOWADtXjp+3buj+6u49z/tiPnAaMJR0HC8C3l85hv4MHEJKfmcB\nd+R5Q4B7gX8mfa42B/bP8yaTPhd/RUrWXwVu6+L9G8O6n9dr8vYNA95MOh4/W9n3LwP/Pb/+3wNP\n8Nr54XbSyX8osD/pi83lnb1Os/Ly6z8P7JaX3QnYo+7zpZukuvYE6aTU6GXSm7NLRLwcEb+O/I51\n4/SIeCEi/tTF/Msi4oGIeAE4Ffh4R6f4BjoS+E5ELIqINcCXgCkNzSBnRMSfIuJe0gds78ZCcixT\ngC9FxOqIWEz6ZvqpDYzv9oj4WUS82s2+aXReRDwREU8DPwfG5+nHApdExA25vGUR8RBARPwiIn4f\nyc3A9aRE35VppNrW0oh4kXRiOizvt8OAayPiljzvVFINpafOi4glHdsdET/K2/VqRFxFqhFM6Gb9\nsyPi2Yh4HJhT2Q89WfbjwLl5O58hNcd25RXSCX+cpM0iYnFE/D7Pq2t/7Qu0RcTXI+KliFhE+oIz\npbLMrRExO1Kf4GW8dvxOICXgf8yfvT9HREcH8jTgrIj4bUSsBf43MF7SLt0FI2kHUnI6KZe5gpSQ\nqvE8FhEX5nhmks4VO+R+vn2B0/K23ArMKtgHnZaX570K7Clpi4hYHhELCsrbIE4YXRtBanJq9C3S\nt5PrJS2SdEpBWUt6MP8xUs1l+6Iou7dzLq9a9qa8dsABVK9q+iOpJtJo+xxTY1kjNjC+ZvulM13F\nO4rUJPI6kiZJukPS05KeJX3ou9u/uwDX5IsgniXVZF4h7bedq3HnJP/UemzHOtsu6dOS7qm85p5N\nYix535otu862NMZUFRELgZNIyWCFpCsl7Zxn17W/dgF27ig3l/1luj9+N8+JahTpZLu2i3LPrZT5\nNOlbe7PjeRfS52B5Zd0LSDWN18UTEX/MD4eT9sPTlWlQdvx3Wl7ej4eTkt9ySb+QtHtBeRvECaMT\nkvYlHTyvu6Qtf8P+h4h4C/Ah4AuS3tsxu4sim9VARlUejybVYlaRmiXeWIlrCNDWg3KfIB3k1bLX\nAk82Wa/RqhxTY1nLCtcv3S/rbC/wuiuuurGE1Oy0DklvIDXznQPsEBHbALNJJ4iuYltCaivepvK3\neUQsI7Vd/+X9kvRGYLtu4mq67fmb7YWkPoTtcowPVGKsy3JSM1KHUV0tCBARV0TE/qTjIIB/yrM2\nZH91954vAR5tKHfLiDikYNuWAKO7uKhgCakZqVruFhFxW0GZLwLbV9bbKiL2KIhnOfCmvP0dqvu7\n2Wf5dSLiuog4kFTreIh0DNXKCaNC0laSDgWuJLUt3t/JModKepskkdrMX+G1KvaTpHbWnvqkpHH5\nYPo68ONcBX2E9I3pA5I2I7W1vqGy3pPAmOolwA1+CPxPSbtKGk6qel/VxbeuLuVYrga+KWnLfIL7\nAqnDusSTwHaStm6y3D3AIZLeJGlH0jfaUhcDx0h6r6RNJI3I37iGkvbZSmCtpElA9bLTzmKbTtrW\nXQAktUmanOf9GDhU0v6ShpLer+4+RyXHxDDSCWNlfr1jSDWMul0NnJj31TakTu1OSdpN0n/LCfjP\nvHaxAGzY/uruPb8TWC3pZElbSBoiac/8ha6ZO0kn6bMlDZO0uaS/qcT7JUl75Hi3lvR3zQqMiOWk\n5sxv53PFJpLeKumAgnUfA+YBp0saKuldwAcri6wk7c+i84ekHSRNljSMlMTWsH5Noz3ihJH8XNJq\n0jeIr5A65o7pYtmxpM7BNaROrO9GxJw87yzgq7m6+sUevP5lpM6/P5A6504AiIjngP8BXET6Nv8C\nUP3NwI/y/6ck3dVJuZfksm8hXUXxZ+DzPYir6vP59ReRal5X5PKbyn0JPwQW5X2zcxeLXkbqR1lM\n+mBeVRpcRNxJes/+mZTIbyb1M60m7c+rSVeafIJK23EXsZ2bl7k+Hxd3APvl5RcAn8vbvzyX2d3v\nOC4mtfs/K+lnXcT+IKlP6HZSgnk78JvSbd8AF5L2833A3aSa11rSl6BGbyD1cawiHadvJvWJwYbt\nry7f8/xF5VBSn8uj+bUvIl2p1K287geBt5EuYFlKasIhIq4h1Y6ulPQ8qTY3qVmZ2adJX0IezNvy\nY9I3/BJHkq5+eop0ddVVpJN9R3PTN4Hf5GPlnU3K2oT0pe0JUpPaAaRO8Vp19N6b2SCXa1/TI6Lb\nzt+N8DqLSVfy3Vjn67Q6SVcBD0XE1/o6llKuYZgNUrmZ5xBJm0oaAXyNdNmo1UDSvrkJaxNJB5Mu\n7+201tmqnDDMBi8BZ5CaVu4mXd10Wp9GNLDtSPqd1BrgPNLvvO7u04h6yE1SZmZWxDUMMzMrUuvA\nZ7md7lzSz9ovioizG+ZPBr5BuhxsLekXlLeWrNuZ7bffPsaMGbNRt8HMbCCbP3/+qohoa75kjU1S\n+Udmj5DG9lkKzAWOyJcQdiwzHHghIkLSXsDVEbF7ybqdaW9vj3nz5tWyPWZmA5Gk+RHRXrJsnU1S\nE4CFkcYxeon0Y7jJ1QUiYk28lrE6frxUtK6ZmfWuOhPGCNYdK2UpnYzVIukjkh4CfkEaXbV43bz+\nVEnzJM1buXLlRgnczMxer887vSPimojYnTTMddO72nWy/oyIaI+I9ra2omY4MzNbD3UmjGWsO7jW\nSLoZrC4ibgHeImn7nq5rZmb1qzNhzAXG5oHvhpLGjF9n/PfKIH5I2oc0Xs1TJeuamVnvqu2y2ohY\nK+l40u0Ph5BubrNA0rQ8fzrwMeDTkl4mjX55eO4E73TdumI1M7PmBtQvvX1ZrZlZz7TKZbVmZjaA\nOGGYmVmRWocG6U8mzpzY6fQ5R83pdLqZ2WDjGoaZmRVxwjAzsyJOGGZmVsQJw8zMijhhmJlZEScM\nMzMr4oRhZmZFnDDMzKyIE4aZmRVxwjAzsyJOGGZmVsQJw8zMijhhmJlZEScMMzMr4oRhZmZFnDDM\nzKyIE4aZmRVxwjAzsyJOGGZmVsQJw8zMijhhmJlZEScMMzMr4oRhZmZFak0Ykg6W9LCkhZJO6WT+\nkZLuk3S/pNsk7V2ZtzhPv0fSvDrjNDOz5jatq2BJQ4DzgQOBpcBcSbMi4sHKYo8CB0TEM5ImATOA\n/SrzJ0bEqrpiNDOzcnXWMCYACyNiUUS8BFwJTK4uEBG3RcQz+ekdwMga4zEzsw1QZ8IYASypPF+a\np3XlWOCXlecB3ChpvqSpNcRnZmY9UFuTVE9ImkhKGPtXJu8fEcskvRm4QdJDEXFLJ+tOBaYCjB49\nulfiNTMbjOqsYSwDRlWej8zT1iFpL+AiYHJEPNUxPSKW5f8rgGtITVyvExEzIqI9Itrb2to2Yvhm\nZlZVZ8KYC4yVtKukocAUYFZ1AUmjgZ8Cn4qIRyrTh0nasuMxcBDwQI2xmplZE7U1SUXEWknHA9cB\nQ4BLImKBpGl5/nTgNGA74LuSANZGRDuwA3BNnrYpcEVE/KquWNfHxJkTO50+56g5vRyJmVnvqLUP\nIyJmA7Mbpk2vPD4OOK6T9RYBezdONzOzvuNfepuZWREnDDMzK+KEYWZmRZwwzMysiBOGmZkVccIw\nM7MiThhmZlbECcPMzIo4YZiZWREnDDMzK+KEYWZmRZwwzMysiBOGmZkVccIwM7MiThhmZlbECcPM\nzIo4YZiZWREnDDMzK+KEYWZmRZwwzMysiBOGmZkVccIwM7MiThhmZlZk074OYLCYOHNip9PnHDWn\nlyMxM1s/rmGYmVkRJwwzMyvihGFmZkVqTRiSDpb0sKSFkk7pZP6Rku6TdL+k2yTtXbqumZn1rtoS\nhqQhwPnAJGAccISkcQ2LPQocEBFvB74BzOjBumZm1ovqrGFMABZGxKKIeAm4EphcXSAibouIZ/LT\nO4CRpeuamVnvqjNhjACWVJ4vzdO6cizwy56uK2mqpHmS5q1cuXIDwjUzs+60RKe3pImkhHFyT9eN\niBkR0R4R7W1tbRs/ODMzA+r94d4yYFTl+cg8bR2S9gIuAiZFxFM9WdfMzHpPnTWMucBYSbtKGgpM\nAWZVF5A0Gvgp8KmIeKQn65qZWe+qrYYREWslHQ9cBwwBLomIBZKm5fnTgdOA7YDvSgJYm5uXOl23\nrljNzKy5WseSiojZwOyGadMrj48Djitd18zM+k5LdHqbmVnrc8IwM7MiThhmZlbECcPMzIo4YZiZ\nWZGiq6Qk/RS4GPhlRLxab0gGvkOfmbWe0hrGd4FPAL+TdLak3WqMyczMWlBRwoiIGyPiSGAfYDFw\nY75/xTGSNqszQDMzaw3FfRiStgOOJv3Q7m7gXFICuaGWyMzMrKWU9mFcA+wGXAZ8MCKW51lXSZpX\nV3BmZtY6SocGuTAP1fEXkt4QES9GRHsNcZmZWYspbZI6s5Npt2/MQMzMrLV1W8OQtCPpTndbSPpr\nQHnWVsAba47NzMxaSLMmqfeTOrpHAt+pTF8NfLmmmMzMrAV1mzAiYiYwU9LHIuInvRSTmZm1oGZN\nUp+MiMuBMZK+0Dg/Ir7TyWpmZjYANWuSGpb/D687EDMza23NmqQuyP/P6J1wzMysVTVrkjqvu/kR\nccLGDcfMzFpVsyap+b0ShZmZtbySq6TMzMyaNkn9S0ScJOnnQDTOj4gP1RaZmZm1lGZNUpfl/+fU\nHYiZmbW2Zk1S8/P/myUNBXYn1TQejoiXeiE+K9TVHfrAd+kzs42jdHjzDwDTgd+TxpPaVdJnI+KX\ndQZnZmato3R4828DEyNiIYCktwK/AJwwzMwGidLhzVd3JItsEWkAQjMzGyS6TRiSPirpo8A8SbMl\nHS3pKODnwNxmhUs6WNLDkhZKOqWT+btLul3Si5K+2DBvsaT7Jd3ju/qZmfW9Zk1SH6w8fhI4ID9e\nCWzR3YqShgDnAwcCS4G5kmZFxIOVxZ4GTgA+3EUxEyNiVZMYzcysFzS7SuqYDSh7ArAwIhYBSLoS\nmAz8JWFExApgRe5UNzOzFlZ6ldTmwLHAHsDmHdMj4jPdrDYCWFJ5vhTYrwexBXCjpFeACyJiRhex\nTQWmAowePboHxZuZWU+UdnpfBuxIugPfzaQ78NXd6b1/RIwHJgGfk/TuzhaKiBkR0R4R7W1tbTWH\nZGY2eJUmjLdFxKnAC3l8qQ/QvLawDBhVeT4yTysSEcvy/xXANaQmLjMz6yOlCePl/P9ZSXsCWwNv\nbrLOXGCspF3zr8SnALNKXkzSMElbdjwGDgIeKIzVzMxqUPrDvRmStgVOJZ30h+fHXYqItZKOB64D\nhgCXRMQCSdPy/OmSdgTmAVsBr0o6CRgHbA9cI6kjxisi4lc93jozM9toihJGRFyUH94MvKW08IiY\nDcxumDa98vgPpKaqRs8De5e+jq2frsaf8thTZtaZoiYpSdtJ+ldJd0maL+lfJG1Xd3BmZtY6Svsw\nrgRWAB8DDgNWAVfVFZSZmbWe0j6MnSLiG5XnZ0o6vI6AzMysNZXWMK6XNEXSJvnv46TObDMzGySa\n3aJ1NekX1wJOAi7PszYB1gBf7GJVMzMbYJqNJbVlbwViZmatrbQPA0kfAjqG57gpIq6tJyQzM2tF\npZfVng2cSBpp9kHgREln1RmYmZm1ltIaxiHA+Ih4FUDSTOBu4Et1BWZmZq2l9CopgG0qj7fe2IGY\nmVlrK61hnAXcLWkO6YqpdwOvu+WqmZkNXE0ThtIIgLcC7wT2zZNPzuNAmZnZINE0YURESJodEW+n\ncHhyMzMbeEr7MO6StG/zxczMbKAq7cPYD/ikpMXAC6R+jIiIveoKzPoPD5NuNjiUJoz31xqFmZm1\nvGZjSW0OTAPeBtwPXBwRa3sjMDMzay3N+jBmAu2kZDEJ+HbtEZmZWUtq1iQ1Ll8dhaSLgTvrD8nM\nzFpRsxrGyx0P3BRlZja4Nath7C3p+fxYwBb5ecdVUlvVGp2ZmbWMZvfDGNJbgZiZWWvryeCDZmY2\niDlhmJlZEScMMzMr4oRhZmZFnDDMzKxI6VhS60XSwcC5wBDgoog4u2H+7sD3gX2Ar0TEOaXrWv/l\nwQrN+qfaahiShgDnk4YUGQccIWlcw2JPAycA56zHumZm1ovqbJKaACyMiEUR8RJwJTC5ukBErIiI\nuVR+UV66rpmZ9a46m6RGAEsqz5eS7quxUdeVNBWYCjB69OieR2lm1k/0dXNuv+/0jogZEdEeEe1t\nbW19HY6Z2YBVZ8JYBoyqPB+Zp9W9rpmZ1aDOhDEXGCtpV0lDgSnArF5Y18zMalBbH0ZErJV0PHAd\n6dLYSyJigaRpef50STsC84CtgFclnUS6B8fzna1bV6xmZtZcrb/DiIjZwOyGadMrj/9Aam4qWtfM\nzPpOv+/0NjOz3uGEYWZmRWptkjLbGPr62nMzS1zDMDOzIk4YZmZWxAnDzMyKOGGYmVkRJwwzMyvi\nhGFmZkV8Wa0NOF1dhgu+FNdsQ7iGYWZmRZwwzMysiBOGmZkVccIwM7MiThhmZlbECcPMzIo4YZiZ\nWRH/DsMMD6FuVsI1DDMzK+KEYWZmRZwwzMysiBOGmZkVccIwM7MivkrKbD34qiobjFzDMDOzIk4Y\nZmZWxAnDzMyK1NqHIelg4FxgCHBRRJzdMF95/iHAH4GjI+KuPG8xsBp4BVgbEe11xmrWF9wXYv1J\nbQlD0hDgfOBAYCkwV9KsiHiwstgkYGz+2w/4Xv7fYWJErKorRjMzK1dnk9QEYGFELIqIl4ArgckN\ny0wGLo3kDmAbSTvVGJOZma2nOhPGCGBJ5fnSPK10mQBulDRf0tSuXkTSVEnzJM1buXLlRgjbzMw6\n08qd3vtHxHhSs9XnJL27s4UiYkZEtEdEe1tbW+9GaGY2iNSZMJYBoyrPR+ZpRctERMf/FcA1pCYu\nMzPrI3VeJTUXGCtpV1ISmAJ8omGZWcDxkq4kdXY/FxHLJQ0DNomI1fnxQcDXa4zVrF/wVVXWl2pL\nGBGxVtLxwHWky2oviYgFkqbl+dOB2aRLaheSLqs9Jq++A3BNuuqWTYErIuJXdcVqZmbN1fo7jIiY\nTUoK1WnTK48D+Fwn6y0C9q4zNjMz65lW7vQ2M7MW4oRhZmZFPLy52QDmTnLbmFzDMDOzIk4YZmZW\nxAnDzMyKuA/DzNbhfg/rimsYZmZWxDUMM9sgrpEMHq5hmJlZEScMMzMr4iYpM+tVbsLqv5wwzKxf\ncuLpfW6SMjOzIk4YZmZWxAnDzMyKOGGYmVkRd3qb2aCwPp3k7lhflxOGmdlGMtATjJukzMysiGsY\nZmZ9pL/VSFzDMDOzIk4YZmZWxAnDzMyKOGGYmVkRJwwzMyvihGFmZkVqTRiSDpb0sKSFkk7pZL4k\nnZfn3ydpn9J1zcysd9WWMCQNAc4HJgHjgCMkjWtYbBIwNv9NBb7Xg3XNzKwX1VnDmAAsjIhFEfES\ncCUwuWGZycClkdwBbCNpp8J1zcysFyki6ilYOgw4OCKOy88/BewXEcdXlrkWODsibs3P/y9wMjCm\n2bqVMqaSaicAuwEP17JBfWt7YFVfB9EivC8S74fE+yHZkP2wS0S0lSzY74cGiYgZwIy+jqNOkuZF\nRHtfx9EKvC8S74fE+yHprf1QZ8JYBoyqPB+Zp5Uss1nBumZm1ovq7MOYC4yVtKukocAUYFbDMrOA\nT+erpd4JPBcRywvXNTOzXlRbDSMi1ko6HrgOGAJcEhELJE3L86cDs4FDgIXAH4Fjulu3rlj7gQHd\n5NZD3heJ90Pi/ZD0yn6ordPbzMwGFv/S28zMijhhmJlZESeMFiPpEkkrJD1QmfYmSTdI+l3+v21f\nxtgbutgPp0taJume/HdIX8bYGySNkjRH0oOSFkg6MU8fVMdEN/thMB4Tm0u6U9K9eV+ckafXfky4\nD6PFSHo3sIb0C/g987T/AzwdEWfncbW2jYiT+zLOunWxH04H1kTEOX0ZW2/KIx/sFBF3SdoSmA98\nGDiaQXRMdLMfPs7gOyYEDIuINZI2A24FTgQ+Ss3HhGsYLSYibgGebpg8GZiZH88kfVAGtC72w6AT\nEcsj4q78eDXwW2AEg+yY6GY/DDp5KKU1+elm+S/ohWPCCaN/2CH/PgXgD8AOfRlMH/t8Htn4koHe\nDNNI0hjgr4H/zyA+Jhr2AwzCY0LSEEn3ACuAGyKiV44JJ4x+JlIb4mBtR/we8BZgPLAc+HbfhtN7\nJA0HfgKcFBHPV+cNpmOik/0wKI+JiHglIsaTRsGYIGnPhvm1HBNOGP3Dk7kNt6Mtd0Ufx9MnIuLJ\n/EF5FbiQNKrxgJfbqX8C/HtE/DRPHnTHRGf7YbAeEx0i4llgDnAwvXBMOGH0D7OAo/Ljo4D/6MNY\n+kzHhyH7CPBAV8sOFLmD82LgtxHxncqsQXVMdLUfBukx0SZpm/x4C+BA4CF64ZjwVVItRtIPgfeQ\nhit+Evga8DPgamA08Bjw8YgY0B3CXeyH95CaHgJYDHy20mY7IEnaH/g1cD/wap78ZVL7/aA5JrrZ\nD0cw+I6JvUid2kNIX/qvjoivS9qOmo8JJwwzMyviJikzMyvihGFmZkWcMMzMrIgThpmZFXHCMDOz\nIk4Y1m9J+koerfO+PFLpfn0d04aQ9ANJh9VY/vjqaK55pNcv1vV6NvDUdotWszpJehdwKLBPRLwo\naXtgaB+H1erGA+2kWyOb9ZhrGNZf7QSsiogXASJiVUQ8ASDpHZJuljRf0nWV4RLeke8hcK+kb3Xc\na0PS0ZL+raNgSddKek9+fJCk2yXdJelHeSwjJC2WdEaefr+k3fP04ZK+n6fdJ+lj3ZVTQtI/Spqb\ny+u498EYSb+VdGGuZV2ff/WLpH0rta5vSXpA0lDg68Dhefrhufhxkm6StEjSCev9btig4IRh/dX1\nwChJj0j6rqQD4C/jDf0rcFhEvAO4BPhmXuf7wOcjYu+SF8i1lq8C74uIfYB5wBcqi6zK078HdDTt\nnAo8FxFvj4i9gP9XUE53MRwEjCWNkTQeeEe+Vwh5+vkRsQfwLPCxynZ+Ng9O9wpARLwEnAZcFRHj\nI+KqvOzuwPtz+V/L+8+sU26Ssn4p3zzmHcDfAhOBq/JNY+YBewI3pOGHGAIsz2PvbJPvswFwGTCp\nycu8ExgH/CaXNRS4vTK/YyDA+aSb1wC8D5hSifMZSYc2Kac7B+W/u/Pz4aRE8TjwaETcU4lhTN7O\nLSOio/wrSE13XflFrqW9KGkFaUjspYWx2SDjhGH9VkS8AtwE3CTpftKAa/OBBRHxruqyHYO1dWEt\n69a2N+9YjXSvgSO6WO/F/P8Vuv8sNSunOwLOiogL1pmY7gnxYmXSK8AW61F+Yxk+J1iX3CRl/ZKk\n3SSNrUwaTxpw7WGgLXeKI2kzSXvkYaCfzYPYARxZWXcxMF7SJpJG8doQ2XcAfyPpbbmsYZL+S5PQ\nbgA+V4lz2/Usp8N1wGcqfScjJL25q4Xzdq6uXDE2pTJ7NbBl4euavY4ThvVXw4GZkh6UdB+pyef0\n3FZ/GPBPku4F7gH+a17nGOB8pTuVqVLWb4BHgQeB84COW4GuJN07+4f5NW4ntfl350xg29zRfC8w\nsYflXCBpaf67PSKuJzUr3Z5rUT+m+Un/WODCvJ3DgOfy9DmkTu5qp7dZMY9Wa4NSbtK5NiL2bLJo\nvyNpeMcbLkerAAAASElEQVQ9n3O/zk4RcWIfh2UDgNsrzQaeD0j6Eunz/RipdmO2wVzDMDOzIu7D\nMDOzIk4YZmZWxAnDzMyKOGGYmVkRJwwzMyvyn/tns69LPKQQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f805b936b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot out training data sequence lengths\n",
    "plt.hist(dcaps_sequence_lengths, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Distribution of truncated training sequence lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_debug_sentences(valBatch_y, iteration, val_predictions):\n",
    "    os.makedirs('sentence_validation' + str(iteration))\n",
    "    with open('sentence_validation' + str(iteration) + '/pred_sents.log', 'w') as f11:\n",
    "        for i in range(0,len(val_predictions)):\n",
    "            f11.write(valBatch_sentences[i]+ ',' + str(val_predictions[i]) + '\\n') \n",
    "            \n",
    "    f11.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, ling_features, biases, vocab_size, keep_prob):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Permuting batch_size and n_steps\n",
    "    #print(x.get_shape())\n",
    "    #x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    # print(x.get_shape())\n",
    "    # x = tf.reshape(x, [-1, 300])\n",
    "    # print(x.get_shape())\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    # x = tf.split(0, seq_max_len, x)\n",
    "    # print(len(x), x[0].get_shape())\n",
    "    #x = tf.squeeze(x)\n",
    "    #print(x.get_shape())\n",
    "    \n",
    "    \n",
    "    #need to input dat in: batch_size, n_steps\n",
    "    with tf.device(\"/cpu:0\"): #http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "        embedding_mat = tf.Variable(tf.random_uniform([vocab_size, config.embedding_size], -1.0, 1.0)) #this should be a matrix vocab size by embedding size\n",
    "        embedding_output = tf.nn.embedding_lookup(embedding_mat, x)\n",
    "    #The result of the embedding operation is a 3-dimensional tensor of shape [None, sequence_length, embedding_size].\n",
    "   \n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    if config.num_layers == 1:\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(config.n_hidden)\n",
    "        if config.keep_prob < 1:\n",
    "            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob) #might also add input_keep_prob\n",
    "        #init_state = lstm_cell.zero_state(config.batch_size, tf.float32)    \n",
    "    else:    \n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(config.n_hidden, reuse=tf.get_variable_scope().reuse)\n",
    "        lstm_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(config.num_layers)], state_is_tuple = True)\n",
    "        if config.keep_prob <1:\n",
    "            def lstm_cell():\n",
    "                return tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(config.n_hidden), output_keep_prob=keep_prob)\n",
    "            lstm_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(config.num_layers)], state_is_tuple = True)\n",
    "\n",
    "        #lstm_cell = tf.contrib.rnn.BasicLSTMCell(config.n_hidden, reuse=tf.get_variable_scope().reuse)\n",
    "        \n",
    "        #init_state = lstm_cell.zero_state(config.batch_size, tf.float32)            \n",
    "        \n",
    "                            \n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic\n",
    "    # calculation.\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, embedding_output, dtype=tf.float32, \n",
    "                                sequence_length=seqlen, time_major=False)\n",
    "\n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e, if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    # print('output_shape:', outputs.get_shape())\n",
    "    # print('states_shape:', states.get_shape())\n",
    "    # outputs = tf.pack(outputs)\n",
    "    # tf.Print(outputs, [outputs], message='outputs', summarize=100)\n",
    "    # print(outputs.get_shape())\n",
    "    # outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    #print(outputs.get_shape())\n",
    "    output = last_relevant(outputs, seqlen)\n",
    "    \n",
    "    output= tf.layers.dense(tf.concat([output, ling_features],1), config.n_hidden+config.linguistic_feats) #dense (fully connected) layer here\n",
    "\n",
    "    # output = tf.Print(output, [output], message='last relevant output', summarize=100)\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    #batch_size = tf.shape(outputs)[0]\n",
    "    #print(batch_size.get_shape())\n",
    "    # Start indices for each sample\n",
    "    #index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    #print(index.get_shape())\n",
    "    # Indexing\n",
    "    #outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "    #print(outputs.get_shape())\n",
    "    # Linear activation, using outputs computed above\n",
    "    #print(output.get_shape())\n",
    "    \n",
    "    return tf.matmul(output, weights['out']) + biases['out'] #ADD LIWC FEAURES IN HERE BY CONCATENATING LSTM OUTPUT AND LIWC FEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = dynamicRNN(x, seqlen, weights, ling_features, biases, vocab_size, keep_prob) #returns a tensor [#] with LOGITS for each class, only become probabilities when fed into softmax function\n",
    "#pred = tf.Print(pred, [weights['out']], message='weights', summarize=10)\n",
    "#print(pred.get_shape(), y.get_shape())\n",
    "#pred = tf.Print(pred, [pred], message='predictions', summarize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prob = tf.nn.softmax(pred) #get probabilities as a variable, so they can be added in and analyzed later\n",
    "#predictions = tf.cast(tf.argmax(pred,1), tf.int64) #a_note, returns the index with the largest value across axis of a tensor, i.e., the predicted class as 1/2/3\n",
    "#labels = tf.cast(tf.argmax(y,1), tf.int64) #a_note, returns the index with the largest value across axis of a tensor, i.e., the expected class as 1/2/3\n",
    "#conf_mat = tf.contrib.metrics.confusion_matrix(predictions, labels) #confusion matrix, where each row is a prediction, each column a true label\n",
    "\n",
    "###################################\n",
    "\n",
    "# Define loss and optimizer\n",
    "\n",
    "#FOR NON-COST SENSITIVE LEARNING:\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) \n",
    "\n",
    "######L2 REGULARIZATION. But if config.L2_penalty is 0 this reduces to no L2 regularization: Not 100% confident in code yet, so keep at 0. \n",
    "reg= tf.nn.l2_loss(weights['out'])\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred) + config.L2_penalty*reg) # this loss in middle is the addded loss plus L2 penalty term. why? shouldnt the mean cost be added to the regualrized mean weights?\n",
    "\n",
    "cost =  tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y, pred)))) + config.L2_penalty*reg\n",
    "\n",
    "#OPTIMIZER\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=config.learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "# Evaluate model\n",
    "#pred = tf.Print(pred, [tf.argmax(pred, 1), tf.argmax(y,1)], message='crisp_prediction', summarize=100)\n",
    "#correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy =   tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(y, pred)))) \n",
    "\n",
    "\n",
    "# Initializing the variables\n",
    "init =  tf.global_variables_initializer() #tf.initialize_all_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Configuration: ----------\n",
      "learning_rate 0.0001\n",
      "training_iters 30000000\n",
      "batch_size Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "L2_penalty 0.0\n",
      "seq_max_len 30\n",
      "seq_min_len 7\n",
      "embedding_size 20\n",
      "n_hidden 15\n",
      "n_classes 1\n",
      "num_layers 1\n",
      "keep_prob (dropout = 1-keep_prob) 0.9\n",
      "------------------------------------\n",
      "46209 46209 46209\n",
      "Iter 250, Minibatch Loss= 2.102056, Training Accuracy= 2.09916, Moving Average Loss= 2.102056, Moving Average Acc= 2.09916\n",
      "Iter 500, Minibatch Loss= 1.894093, Training Accuracy= 1.89011, Moving Average Loss= 2.099976, Moving Average Acc= 2.09707\n",
      "Iter 750, Minibatch Loss= 1.547582, Training Accuracy= 1.53548, Moving Average Loss= 2.094452, Moving Average Acc= 2.09145\n",
      "Iter 1000, Minibatch Loss= 2.092054, Training Accuracy= 2.08557, Moving Average Loss= 2.094428, Moving Average Acc= 2.09139\n",
      "Iter 1250, Minibatch Loss= 1.328581, Training Accuracy= 1.30082, Moving Average Loss= 2.086770, Moving Average Acc= 2.08349\n",
      "Iter 1500, Minibatch Loss= 1.219465, Training Accuracy= 1.22317, Moving Average Loss= 2.078097, Moving Average Acc= 2.07488\n",
      "Iter 1750, Minibatch Loss= 1.217888, Training Accuracy= 1.21967, Moving Average Loss= 2.069495, Moving Average Acc= 2.06633\n",
      "Iter 2000, Minibatch Loss= 1.310689, Training Accuracy= 1.32382, Moving Average Loss= 2.061907, Moving Average Acc= 2.05891\n",
      "Iter 2250, Minibatch Loss= 1.108101, Training Accuracy= 1.10393, Moving Average Loss= 2.052369, Moving Average Acc= 2.04936\n",
      "Iter 2500, Minibatch Loss= 0.991241, Training Accuracy= 0.96216, Moving Average Loss= 2.041757, Moving Average Acc= 2.03848\n",
      "Iter 2750, Minibatch Loss= 0.993969, Training Accuracy= 1.01175, Moving Average Loss= 2.031280, Moving Average Acc= 2.02822\n",
      "Iter 3000, Minibatch Loss= 0.871622, Training Accuracy= 0.87001, Moving Average Loss= 2.019683, Moving Average Acc= 2.01663\n",
      "Iter 3250, Minibatch Loss= 0.937649, Training Accuracy= 0.93190, Moving Average Loss= 2.008863, Moving Average Acc= 2.00579\n",
      "Iter 3500, Minibatch Loss= 0.928856, Training Accuracy= 0.92107, Moving Average Loss= 1.998063, Moving Average Acc= 1.99494\n",
      "Iter 3750, Minibatch Loss= 0.963168, Training Accuracy= 0.97640, Moving Average Loss= 1.987714, Moving Average Acc= 1.98475\n",
      "Iter 4000, Minibatch Loss= 0.912406, Training Accuracy= 0.90914, Moving Average Loss= 1.976961, Moving Average Acc= 1.97400\n",
      "Iter 4250, Minibatch Loss= 0.931552, Training Accuracy= 0.92902, Moving Average Loss= 1.966506, Moving Average Acc= 1.96355\n",
      "Iter 4500, Minibatch Loss= 0.885985, Training Accuracy= 0.89838, Moving Average Loss= 1.955701, Moving Average Acc= 1.95290\n",
      "Iter 4750, Minibatch Loss= 0.892695, Training Accuracy= 0.90873, Moving Average Loss= 1.945071, Moving Average Acc= 1.94246\n",
      "Iter 5000, Minibatch Loss= 0.908165, Training Accuracy= 0.92398, Moving Average Loss= 1.934702, Moving Average Acc= 1.93227\n",
      "Iter 5250, Minibatch Loss= 0.801432, Training Accuracy= 0.80397, Moving Average Loss= 1.923369, Moving Average Acc= 1.92099\n",
      "Iter 5500, Minibatch Loss= 0.874891, Training Accuracy= 0.87158, Moving Average Loss= 1.912885, Moving Average Acc= 1.91049\n",
      "Iter 5750, Minibatch Loss= 1.007760, Training Accuracy= 1.01513, Moving Average Loss= 1.903833, Moving Average Acc= 1.90154\n",
      "Iter 6000, Minibatch Loss= 0.862066, Training Accuracy= 0.86479, Moving Average Loss= 1.893416, Moving Average Acc= 1.89117\n",
      "Iter 6250, Minibatch Loss= 0.861420, Training Accuracy= 0.87865, Moving Average Loss= 1.883096, Moving Average Acc= 1.88105\n",
      "Iter 6500, Minibatch Loss= 0.861288, Training Accuracy= 0.83991, Moving Average Loss= 1.872878, Moving Average Acc= 1.87064\n",
      "Iter 6750, Minibatch Loss= 0.859144, Training Accuracy= 0.85158, Moving Average Loss= 1.862740, Moving Average Acc= 1.86045\n",
      "Iter 7000, Minibatch Loss= 0.895579, Training Accuracy= 0.89930, Moving Average Loss= 1.853069, Moving Average Acc= 1.85083\n",
      "Iter 7250, Minibatch Loss= 0.841592, Training Accuracy= 0.83780, Moving Average Loss= 1.842954, Moving Average Acc= 1.84070\n",
      "Iter 7500, Minibatch Loss= 0.841461, Training Accuracy= 0.85120, Moving Average Loss= 1.832939, Moving Average Acc= 1.83081\n",
      "Iter 7750, Minibatch Loss= 0.851949, Training Accuracy= 0.85414, Moving Average Loss= 1.823129, Moving Average Acc= 1.82104\n",
      "Iter 8000, Minibatch Loss= 0.887699, Training Accuracy= 0.88810, Moving Average Loss= 1.813775, Moving Average Acc= 1.81171\n",
      "Iter 8250, Minibatch Loss= 0.807949, Training Accuracy= 0.82286, Moving Average Loss= 1.803717, Moving Average Acc= 1.80182\n",
      "Iter 8500, Minibatch Loss= 0.864699, Training Accuracy= 0.86433, Moving Average Loss= 1.794326, Moving Average Acc= 1.79245\n",
      "Iter 8750, Minibatch Loss= 0.912502, Training Accuracy= 0.91158, Moving Average Loss= 1.785508, Moving Average Acc= 1.78364\n",
      "Iter 9000, Minibatch Loss= 0.865759, Training Accuracy= 0.86979, Moving Average Loss= 1.776311, Moving Average Acc= 1.77450\n",
      "Iter 9250, Minibatch Loss= 0.885606, Training Accuracy= 0.90991, Moving Average Loss= 1.767404, Moving Average Acc= 1.76586\n",
      "Iter 9500, Minibatch Loss= 0.852087, Training Accuracy= 0.84659, Moving Average Loss= 1.758250, Moving Average Acc= 1.75666\n",
      "Iter 9750, Minibatch Loss= 0.860936, Training Accuracy= 0.88763, Moving Average Loss= 1.749277, Moving Average Acc= 1.74797\n",
      "Iter 10000, Minibatch Loss= 0.813660, Training Accuracy= 0.79983, Moving Average Loss= 1.739921, Moving Average Acc= 1.73849\n",
      "Iter 10250, Minibatch Loss= 0.814066, Training Accuracy= 0.79393, Moving Average Loss= 1.730663, Moving Average Acc= 1.72905\n",
      "Iter 10500, Minibatch Loss= 0.954991, Training Accuracy= 0.94254, Moving Average Loss= 1.722906, Moving Average Acc= 1.72118\n",
      "Iter 10750, Minibatch Loss= 0.829897, Training Accuracy= 0.82892, Moving Average Loss= 1.713976, Moving Average Acc= 1.71226\n",
      "Iter 11000, Minibatch Loss= 0.858082, Training Accuracy= 0.84352, Moving Average Loss= 1.705417, Moving Average Acc= 1.70357\n",
      "Iter 11250, Minibatch Loss= 0.845888, Training Accuracy= 0.82376, Moving Average Loss= 1.696822, Moving Average Acc= 1.69477\n",
      "Iter 11500, Minibatch Loss= 0.807817, Training Accuracy= 0.79389, Moving Average Loss= 1.687932, Moving Average Acc= 1.68576\n",
      "Iter 11750, Minibatch Loss= 0.880517, Training Accuracy= 0.86805, Moving Average Loss= 1.679857, Moving Average Acc= 1.67759\n",
      "Iter 12000, Minibatch Loss= 0.871556, Training Accuracy= 0.87954, Moving Average Loss= 1.671774, Moving Average Acc= 1.66961\n",
      "Iter 12250, Minibatch Loss= 0.862877, Training Accuracy= 0.86365, Moving Average Loss= 1.663685, Moving Average Acc= 1.66155\n",
      "Iter 12500, Minibatch Loss= 0.851538, Training Accuracy= 0.84352, Moving Average Loss= 1.655564, Moving Average Acc= 1.65337\n",
      "Testing Accuracy: 0.814139\n",
      "Iter 12750, Minibatch Loss= 0.865677, Training Accuracy= 0.86504, Moving Average Loss= 1.647665, Moving Average Acc= 1.64548\n",
      "Iter 13000, Minibatch Loss= 0.870776, Training Accuracy= 0.85752, Moving Average Loss= 1.639896, Moving Average Acc= 1.63760\n",
      "Iter 13250, Minibatch Loss= 0.783630, Training Accuracy= 0.78406, Moving Average Loss= 1.631333, Moving Average Acc= 1.62907\n",
      "Iter 13500, Minibatch Loss= 0.817559, Training Accuracy= 0.81467, Moving Average Loss= 1.623196, Moving Average Acc= 1.62092\n",
      "Iter 13750, Minibatch Loss= 0.793312, Training Accuracy= 0.80836, Moving Average Loss= 1.614897, Moving Average Acc= 1.61280\n",
      "Iter 14000, Minibatch Loss= 0.819294, Training Accuracy= 0.80973, Moving Average Loss= 1.606941, Moving Average Acc= 1.60477\n",
      "Iter 14250, Minibatch Loss= 0.843783, Training Accuracy= 0.86853, Moving Average Loss= 1.599309, Moving Average Acc= 1.59741\n",
      "Iter 14500, Minibatch Loss= 0.813745, Training Accuracy= 0.80061, Moving Average Loss= 1.591454, Moving Average Acc= 1.58944\n",
      "Iter 14750, Minibatch Loss= 0.819207, Training Accuracy= 0.80500, Moving Average Loss= 1.583731, Moving Average Acc= 1.58159\n",
      "Iter 15000, Minibatch Loss= 0.871743, Training Accuracy= 0.85337, Moving Average Loss= 1.576611, Moving Average Acc= 1.57431\n",
      "Iter 15250, Minibatch Loss= 0.811002, Training Accuracy= 0.81723, Moving Average Loss= 1.568955, Moving Average Acc= 1.56674\n",
      "Iter 15500, Minibatch Loss= 0.772301, Training Accuracy= 0.76170, Moving Average Loss= 1.560989, Moving Average Acc= 1.55869\n",
      "Iter 15750, Minibatch Loss= 0.754426, Training Accuracy= 0.75974, Moving Average Loss= 1.552923, Moving Average Acc= 1.55070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16000, Minibatch Loss= 0.734598, Training Accuracy= 0.74288, Moving Average Loss= 1.544740, Moving Average Acc= 1.54262\n",
      "Iter 16250, Minibatch Loss= 0.823356, Training Accuracy= 0.81229, Moving Average Loss= 1.537526, Moving Average Acc= 1.53532\n",
      "Iter 16500, Minibatch Loss= 0.735484, Training Accuracy= 0.74673, Moving Average Loss= 1.529506, Moving Average Acc= 1.52743\n",
      "Iter 16750, Minibatch Loss= 0.818966, Training Accuracy= 0.80090, Moving Average Loss= 1.522400, Moving Average Acc= 1.52017\n",
      "Iter 17000, Minibatch Loss= 0.723247, Training Accuracy= 0.73813, Moving Average Loss= 1.514409, Moving Average Acc= 1.51235\n",
      "Iter 17250, Minibatch Loss= 0.776781, Training Accuracy= 0.78603, Moving Average Loss= 1.507032, Moving Average Acc= 1.50508\n",
      "Iter 17500, Minibatch Loss= 0.707598, Training Accuracy= 0.69475, Moving Average Loss= 1.499038, Moving Average Acc= 1.49698\n",
      "Iter 17750, Minibatch Loss= 0.709446, Training Accuracy= 0.71200, Moving Average Loss= 1.491142, Moving Average Acc= 1.48913\n",
      "Iter 18000, Minibatch Loss= 0.719104, Training Accuracy= 0.73148, Moving Average Loss= 1.483422, Moving Average Acc= 1.48155\n",
      "Iter 18250, Minibatch Loss= 0.744313, Training Accuracy= 0.74599, Moving Average Loss= 1.476031, Moving Average Acc= 1.47420\n",
      "Iter 18500, Minibatch Loss= 0.725330, Training Accuracy= 0.74423, Moving Average Loss= 1.468524, Moving Average Acc= 1.46690\n",
      "Iter 18750, Minibatch Loss= 0.756455, Training Accuracy= 0.77190, Moving Average Loss= 1.461403, Moving Average Acc= 1.45995\n",
      "Iter 19000, Minibatch Loss= 0.769947, Training Accuracy= 0.79332, Moving Average Loss= 1.454488, Moving Average Acc= 1.45328\n",
      "Iter 19250, Minibatch Loss= 0.793585, Training Accuracy= 0.78664, Moving Average Loss= 1.447879, Moving Average Acc= 1.44662\n",
      "Iter 19500, Minibatch Loss= 0.724517, Training Accuracy= 0.73004, Moving Average Loss= 1.440646, Moving Average Acc= 1.43945\n",
      "Iter 19750, Minibatch Loss= 0.702209, Training Accuracy= 0.69043, Moving Average Loss= 1.433261, Moving Average Acc= 1.43196\n",
      "Iter 20000, Minibatch Loss= 0.717038, Training Accuracy= 0.69921, Moving Average Loss= 1.426099, Moving Average Acc= 1.42463\n",
      "Iter 20250, Minibatch Loss= 0.748416, Training Accuracy= 0.73106, Moving Average Loss= 1.419322, Moving Average Acc= 1.41770\n",
      "Iter 20500, Minibatch Loss= 0.725052, Training Accuracy= 0.73161, Moving Average Loss= 1.412380, Moving Average Acc= 1.41084\n",
      "Iter 20750, Minibatch Loss= 0.702694, Training Accuracy= 0.69749, Moving Average Loss= 1.405283, Moving Average Acc= 1.40370\n",
      "Iter 21000, Minibatch Loss= 0.740213, Training Accuracy= 0.73194, Moving Average Loss= 1.398632, Moving Average Acc= 1.39698\n",
      "Iter 21250, Minibatch Loss= 0.678738, Training Accuracy= 0.67957, Moving Average Loss= 1.391433, Moving Average Acc= 1.38981\n",
      "Iter 21500, Minibatch Loss= 0.675241, Training Accuracy= 0.67503, Moving Average Loss= 1.384271, Moving Average Acc= 1.38266\n",
      "Iter 21750, Minibatch Loss= 0.711709, Training Accuracy= 0.73011, Moving Average Loss= 1.377546, Moving Average Acc= 1.37614\n",
      "Iter 22000, Minibatch Loss= 0.670952, Training Accuracy= 0.65623, Moving Average Loss= 1.370480, Moving Average Acc= 1.36894\n",
      "Iter 22250, Minibatch Loss= 0.722282, Training Accuracy= 0.72212, Moving Average Loss= 1.363998, Moving Average Acc= 1.36247\n",
      "Iter 22500, Minibatch Loss= 0.666671, Training Accuracy= 0.67048, Moving Average Loss= 1.357024, Moving Average Acc= 1.35555\n",
      "Iter 22750, Minibatch Loss= 0.651641, Training Accuracy= 0.66472, Moving Average Loss= 1.349971, Moving Average Acc= 1.34864\n",
      "Iter 23000, Minibatch Loss= 0.646866, Training Accuracy= 0.64843, Moving Average Loss= 1.342939, Moving Average Acc= 1.34164\n",
      "Iter 23250, Minibatch Loss= 0.629971, Training Accuracy= 0.66650, Moving Average Loss= 1.335810, Moving Average Acc= 1.33489\n",
      "Iter 23500, Minibatch Loss= 0.668315, Training Accuracy= 0.65861, Moving Average Loss= 1.329135, Moving Average Acc= 1.32813\n",
      "Iter 23750, Minibatch Loss= 0.681252, Training Accuracy= 0.68552, Moving Average Loss= 1.322656, Moving Average Acc= 1.32170\n",
      "Iter 24000, Minibatch Loss= 0.730277, Training Accuracy= 0.72928, Moving Average Loss= 1.316732, Moving Average Acc= 1.31578\n",
      "Iter 24250, Minibatch Loss= 0.648682, Training Accuracy= 0.66506, Moving Average Loss= 1.310052, Moving Average Acc= 1.30927\n",
      "Iter 24500, Minibatch Loss= 0.685699, Training Accuracy= 0.68604, Moving Average Loss= 1.303808, Moving Average Acc= 1.30304\n",
      "Iter 24750, Minibatch Loss= 0.621858, Training Accuracy= 0.62212, Moving Average Loss= 1.296989, Moving Average Acc= 1.29623\n",
      "Iter 25000, Minibatch Loss= 0.643466, Training Accuracy= 0.64424, Moving Average Loss= 1.290453, Moving Average Acc= 1.28971\n",
      "Testing Accuracy: 0.649445\n",
      "Iter 25250, Minibatch Loss= 0.634169, Training Accuracy= 0.63279, Moving Average Loss= 1.283891, Moving Average Acc= 1.28314\n",
      "Iter 25500, Minibatch Loss= 0.663171, Training Accuracy= 0.68807, Moving Average Loss= 1.277683, Moving Average Acc= 1.27719\n",
      "Iter 25750, Minibatch Loss= 0.637102, Training Accuracy= 0.63379, Moving Average Loss= 1.271278, Moving Average Acc= 1.27075\n",
      "Iter 26000, Minibatch Loss= 0.619379, Training Accuracy= 0.61370, Moving Average Loss= 1.264759, Moving Average Acc= 1.26418\n",
      "Iter 26250, Minibatch Loss= 0.633565, Training Accuracy= 0.66460, Moving Average Loss= 1.258447, Moving Average Acc= 1.25819\n",
      "Iter 26500, Minibatch Loss= 0.623051, Training Accuracy= 0.62893, Moving Average Loss= 1.252093, Moving Average Acc= 1.25189\n",
      "Iter 26750, Minibatch Loss= 0.674196, Training Accuracy= 0.66307, Moving Average Loss= 1.246314, Moving Average Acc= 1.24601\n",
      "Iter 27000, Minibatch Loss= 0.610640, Training Accuracy= 0.61709, Moving Average Loss= 1.239957, Moving Average Acc= 1.23972\n",
      "Iter 27250, Minibatch Loss= 0.649629, Training Accuracy= 0.66119, Moving Average Loss= 1.234054, Moving Average Acc= 1.23393\n",
      "Iter 27500, Minibatch Loss= 0.577813, Training Accuracy= 0.59909, Moving Average Loss= 1.227491, Moving Average Acc= 1.22758\n",
      "Iter 27750, Minibatch Loss= 0.642486, Training Accuracy= 0.63018, Moving Average Loss= 1.221641, Moving Average Acc= 1.22161\n",
      "Iter 28000, Minibatch Loss= 0.647159, Training Accuracy= 0.65953, Moving Average Loss= 1.215896, Moving Average Acc= 1.21599\n",
      "Iter 28250, Minibatch Loss= 0.617462, Training Accuracy= 0.60544, Moving Average Loss= 1.209912, Moving Average Acc= 1.20988\n",
      "Iter 28500, Minibatch Loss= 0.592524, Training Accuracy= 0.61135, Moving Average Loss= 1.203738, Moving Average Acc= 1.20390\n",
      "Iter 28750, Minibatch Loss= 0.629143, Training Accuracy= 0.62807, Moving Average Loss= 1.197992, Moving Average Acc= 1.19814\n",
      "Iter 29000, Minibatch Loss= 0.651174, Training Accuracy= 0.63859, Moving Average Loss= 1.192524, Moving Average Acc= 1.19254\n",
      "Iter 29250, Minibatch Loss= 0.562664, Training Accuracy= 0.56779, Moving Average Loss= 1.186226, Moving Average Acc= 1.18630\n",
      "Iter 29500, Minibatch Loss= 0.560140, Training Accuracy= 0.55738, Moving Average Loss= 1.179965, Moving Average Acc= 1.18001\n",
      "Iter 29750, Minibatch Loss= 0.602524, Training Accuracy= 0.58532, Moving Average Loss= 1.174190, Moving Average Acc= 1.17406\n",
      "Iter 30000, Minibatch Loss= 0.595509, Training Accuracy= 0.58602, Moving Average Loss= 1.168403, Moving Average Acc= 1.16818\n",
      "Iter 30250, Minibatch Loss= 0.570477, Training Accuracy= 0.59382, Moving Average Loss= 1.162424, Moving Average Acc= 1.16244\n",
      "Iter 30500, Minibatch Loss= 0.581536, Training Accuracy= 0.60275, Moving Average Loss= 1.156615, Moving Average Acc= 1.15684\n",
      "Iter 30750, Minibatch Loss= 0.587469, Training Accuracy= 0.59687, Moving Average Loss= 1.150924, Moving Average Acc= 1.15124\n",
      "Iter 31000, Minibatch Loss= 0.576147, Training Accuracy= 0.58243, Moving Average Loss= 1.145176, Moving Average Acc= 1.14555\n",
      "Iter 31250, Minibatch Loss= 0.569792, Training Accuracy= 0.56205, Moving Average Loss= 1.139422, Moving Average Acc= 1.13972\n",
      "Iter 31500, Minibatch Loss= 0.580272, Training Accuracy= 0.58094, Moving Average Loss= 1.133831, Moving Average Acc= 1.13413\n",
      "Iter 31750, Minibatch Loss= 0.563148, Training Accuracy= 0.56642, Moving Average Loss= 1.128124, Moving Average Acc= 1.12845\n",
      "Iter 32000, Minibatch Loss= 0.545740, Training Accuracy= 0.55689, Moving Average Loss= 1.122300, Moving Average Acc= 1.12274\n",
      "Iter 32250, Minibatch Loss= 0.596313, Training Accuracy= 0.58586, Moving Average Loss= 1.117040, Moving Average Acc= 1.11737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32500, Minibatch Loss= 0.553985, Training Accuracy= 0.54987, Moving Average Loss= 1.111410, Moving Average Acc= 1.11169\n",
      "Iter 32750, Minibatch Loss= 0.574458, Training Accuracy= 0.57009, Moving Average Loss= 1.106040, Moving Average Acc= 1.10628\n",
      "Iter 33000, Minibatch Loss= 0.617181, Training Accuracy= 0.60579, Moving Average Loss= 1.101152, Moving Average Acc= 1.10127\n",
      "Iter 33250, Minibatch Loss= 0.556237, Training Accuracy= 0.55239, Moving Average Loss= 1.095702, Moving Average Acc= 1.09578\n",
      "Iter 33500, Minibatch Loss= 0.568918, Training Accuracy= 0.57010, Moving Average Loss= 1.090435, Moving Average Acc= 1.09053\n",
      "Iter 33750, Minibatch Loss= 0.558316, Training Accuracy= 0.56721, Moving Average Loss= 1.085113, Moving Average Acc= 1.08529\n",
      "Iter 34000, Minibatch Loss= 0.524765, Training Accuracy= 0.53015, Moving Average Loss= 1.079510, Moving Average Acc= 1.07974\n",
      "Iter 34250, Minibatch Loss= 0.544076, Training Accuracy= 0.54805, Moving Average Loss= 1.074156, Moving Average Acc= 1.07442\n",
      "Iter 34500, Minibatch Loss= 0.533150, Training Accuracy= 0.53599, Moving Average Loss= 1.068745, Moving Average Acc= 1.06904\n",
      "Iter 34750, Minibatch Loss= 0.556845, Training Accuracy= 0.54606, Moving Average Loss= 1.063626, Moving Average Acc= 1.06381\n",
      "Iter 35000, Minibatch Loss= 0.476435, Training Accuracy= 0.49069, Moving Average Loss= 1.057755, Moving Average Acc= 1.05808\n",
      "Iter 35250, Minibatch Loss= 0.538798, Training Accuracy= 0.54647, Moving Average Loss= 1.052565, Moving Average Acc= 1.05296\n",
      "Iter 35500, Minibatch Loss= 0.498950, Training Accuracy= 0.50421, Moving Average Loss= 1.047029, Moving Average Acc= 1.04748\n",
      "Iter 35750, Minibatch Loss= 0.528043, Training Accuracy= 0.52935, Moving Average Loss= 1.041839, Moving Average Acc= 1.04229\n",
      "Iter 36000, Minibatch Loss= 0.505248, Training Accuracy= 0.51694, Moving Average Loss= 1.036473, Moving Average Acc= 1.03704\n",
      "Iter 36250, Minibatch Loss= 0.522493, Training Accuracy= 0.51736, Moving Average Loss= 1.031333, Moving Average Acc= 1.03184\n",
      "Iter 36500, Minibatch Loss= 0.527134, Training Accuracy= 0.52764, Moving Average Loss= 1.026291, Moving Average Acc= 1.02680\n",
      "Iter 36750, Minibatch Loss= 0.493764, Training Accuracy= 0.48965, Moving Average Loss= 1.020966, Moving Average Acc= 1.02143\n",
      "Iter 37000, Minibatch Loss= 0.487109, Training Accuracy= 0.49364, Moving Average Loss= 1.015627, Moving Average Acc= 1.01615\n",
      "Iter 37250, Minibatch Loss= 0.475309, Training Accuracy= 0.48801, Moving Average Loss= 1.010224, Moving Average Acc= 1.01087\n",
      "Iter 37500, Minibatch Loss= 0.548610, Training Accuracy= 0.52779, Moving Average Loss= 1.005608, Moving Average Acc= 1.00604\n",
      "Testing Accuracy: 0.490674\n",
      "Iter 37750, Minibatch Loss= 0.493766, Training Accuracy= 0.48891, Moving Average Loss= 1.000490, Moving Average Acc= 1.00087\n",
      "Iter 38000, Minibatch Loss= 0.518858, Training Accuracy= 0.51557, Moving Average Loss= 0.995673, Moving Average Acc= 0.99602\n",
      "Iter 38250, Minibatch Loss= 0.530643, Training Accuracy= 0.53728, Moving Average Loss= 0.991023, Moving Average Acc= 0.99143\n",
      "Iter 38500, Minibatch Loss= 0.497247, Training Accuracy= 0.49559, Moving Average Loss= 0.986085, Moving Average Acc= 0.98647\n",
      "Iter 38750, Minibatch Loss= 0.485384, Training Accuracy= 0.49276, Moving Average Loss= 0.981078, Moving Average Acc= 0.98153\n",
      "Iter 39000, Minibatch Loss= 0.504389, Training Accuracy= 0.50568, Moving Average Loss= 0.976311, Moving Average Acc= 0.97677\n",
      "Iter 39250, Minibatch Loss= 0.447503, Training Accuracy= 0.45547, Moving Average Loss= 0.971023, Moving Average Acc= 0.97156\n",
      "Iter 39500, Minibatch Loss= 0.465168, Training Accuracy= 0.45103, Moving Average Loss= 0.965965, Moving Average Acc= 0.96636\n",
      "Iter 39750, Minibatch Loss= 0.495269, Training Accuracy= 0.49998, Moving Average Loss= 0.961258, Moving Average Acc= 0.96169\n",
      "Iter 40000, Minibatch Loss= 0.465006, Training Accuracy= 0.48234, Moving Average Loss= 0.956295, Moving Average Acc= 0.95690\n",
      "Iter 40250, Minibatch Loss= 0.453311, Training Accuracy= 0.43667, Moving Average Loss= 0.951265, Moving Average Acc= 0.95170\n",
      "Iter 40500, Minibatch Loss= 0.486477, Training Accuracy= 0.49016, Moving Average Loss= 0.946618, Moving Average Acc= 0.94708\n",
      "Iter 40750, Minibatch Loss= 0.467753, Training Accuracy= 0.47144, Moving Average Loss= 0.941829, Moving Average Acc= 0.94232\n",
      "Iter 41000, Minibatch Loss= 0.470855, Training Accuracy= 0.45329, Moving Average Loss= 0.937119, Moving Average Acc= 0.93743\n",
      "Iter 41250, Minibatch Loss= 0.477911, Training Accuracy= 0.46153, Moving Average Loss= 0.932527, Moving Average Acc= 0.93268\n",
      "Iter 41500, Minibatch Loss= 0.460653, Training Accuracy= 0.46758, Moving Average Loss= 0.927808, Moving Average Acc= 0.92802\n",
      "Iter 41750, Minibatch Loss= 0.451452, Training Accuracy= 0.44343, Moving Average Loss= 0.923045, Moving Average Acc= 0.92318\n",
      "Iter 42000, Minibatch Loss= 0.446350, Training Accuracy= 0.45158, Moving Average Loss= 0.918278, Moving Average Acc= 0.91846\n",
      "Iter 42250, Minibatch Loss= 0.441484, Training Accuracy= 0.42635, Moving Average Loss= 0.913510, Moving Average Acc= 0.91354\n",
      "Iter 42500, Minibatch Loss= 0.447210, Training Accuracy= 0.42949, Moving Average Loss= 0.908847, Moving Average Acc= 0.90870\n",
      "Iter 42750, Minibatch Loss= 0.448278, Training Accuracy= 0.43128, Moving Average Loss= 0.904241, Moving Average Acc= 0.90393\n",
      "Iter 43000, Minibatch Loss= 0.451444, Training Accuracy= 0.45513, Moving Average Loss= 0.899713, Moving Average Acc= 0.89944\n",
      "Iter 43250, Minibatch Loss= 0.391791, Training Accuracy= 0.38521, Moving Average Loss= 0.894634, Moving Average Acc= 0.89430\n",
      "Iter 43500, Minibatch Loss= 0.442429, Training Accuracy= 0.44840, Moving Average Loss= 0.890112, Moving Average Acc= 0.88984\n",
      "Iter 43750, Minibatch Loss= 0.476174, Training Accuracy= 0.48419, Moving Average Loss= 0.885973, Moving Average Acc= 0.88578\n",
      "Iter 44000, Minibatch Loss= 0.410632, Training Accuracy= 0.41656, Moving Average Loss= 0.881219, Moving Average Acc= 0.88109\n",
      "Iter 44250, Minibatch Loss= 0.470096, Training Accuracy= 0.47213, Moving Average Loss= 0.877108, Moving Average Acc= 0.87700\n",
      "Iter 44500, Minibatch Loss= 0.426581, Training Accuracy= 0.42154, Moving Average Loss= 0.872603, Moving Average Acc= 0.87244\n",
      "Iter 44750, Minibatch Loss= 0.434175, Training Accuracy= 0.43841, Moving Average Loss= 0.868218, Moving Average Acc= 0.86810\n",
      "Iter 45000, Minibatch Loss= 0.439855, Training Accuracy= 0.44208, Moving Average Loss= 0.863935, Moving Average Acc= 0.86384\n",
      "Iter 45250, Minibatch Loss= 0.411049, Training Accuracy= 0.40733, Moving Average Loss= 0.859406, Moving Average Acc= 0.85928\n",
      "Iter 45500, Minibatch Loss= 0.438338, Training Accuracy= 0.44096, Moving Average Loss= 0.855195, Moving Average Acc= 0.85510\n",
      "Iter 45750, Minibatch Loss= 0.398132, Training Accuracy= 0.40091, Moving Average Loss= 0.850625, Moving Average Acc= 0.85055\n",
      "Iter 46000, Minibatch Loss= 0.398862, Training Accuracy= 0.39067, Moving Average Loss= 0.846107, Moving Average Acc= 0.84595\n",
      "Iter 46250, Minibatch Loss= 0.446375, Training Accuracy= 0.43899, Moving Average Loss= 0.842110, Moving Average Acc= 0.84189\n",
      "Iter 46500, Minibatch Loss= 0.412269, Training Accuracy= 0.41621, Moving Average Loss= 0.837811, Moving Average Acc= 0.83763\n",
      "Iter 46750, Minibatch Loss= 0.380440, Training Accuracy= 0.38248, Moving Average Loss= 0.833238, Moving Average Acc= 0.83308\n",
      "Iter 47000, Minibatch Loss= 0.415758, Training Accuracy= 0.41979, Moving Average Loss= 0.829063, Moving Average Acc= 0.82894\n",
      "Iter 47250, Minibatch Loss= 0.403701, Training Accuracy= 0.40935, Moving Average Loss= 0.824809, Moving Average Acc= 0.82475\n",
      "Iter 47500, Minibatch Loss= 0.426521, Training Accuracy= 0.42878, Moving Average Loss= 0.820826, Moving Average Acc= 0.82079\n",
      "Iter 47750, Minibatch Loss= 0.413560, Training Accuracy= 0.41076, Moving Average Loss= 0.816754, Moving Average Acc= 0.81669\n",
      "Iter 48000, Minibatch Loss= 0.395157, Training Accuracy= 0.39567, Moving Average Loss= 0.812538, Moving Average Acc= 0.81248\n",
      "Iter 48250, Minibatch Loss= 0.408612, Training Accuracy= 0.40661, Moving Average Loss= 0.808498, Moving Average Acc= 0.80842\n",
      "Iter 48500, Minibatch Loss= 0.393322, Training Accuracy= 0.39788, Moving Average Loss= 0.804347, Moving Average Acc= 0.80431\n",
      "Iter 48750, Minibatch Loss= 0.396716, Training Accuracy= 0.40835, Moving Average Loss= 0.800270, Moving Average Acc= 0.80035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 49000, Minibatch Loss= 0.418634, Training Accuracy= 0.42126, Moving Average Loss= 0.796454, Moving Average Acc= 0.79656\n",
      "Iter 49250, Minibatch Loss= 0.393528, Training Accuracy= 0.39395, Moving Average Loss= 0.792425, Moving Average Acc= 0.79254\n",
      "Iter 49500, Minibatch Loss= 0.405522, Training Accuracy= 0.39931, Moving Average Loss= 0.788556, Moving Average Acc= 0.78860\n",
      "Iter 49750, Minibatch Loss= 0.412536, Training Accuracy= 0.40200, Moving Average Loss= 0.784795, Moving Average Acc= 0.78474\n",
      "Iter 50000, Minibatch Loss= 0.425413, Training Accuracy= 0.40572, Moving Average Loss= 0.781202, Moving Average Acc= 0.78095\n",
      "Testing Accuracy: 0.380744\n",
      "Iter 50250, Minibatch Loss= 0.388581, Training Accuracy= 0.39123, Moving Average Loss= 0.777275, Moving Average Acc= 0.77705\n",
      "Iter 50500, Minibatch Loss= 0.402058, Training Accuracy= 0.40545, Moving Average Loss= 0.773523, Moving Average Acc= 0.77334\n",
      "Iter 50750, Minibatch Loss= 0.386778, Training Accuracy= 0.37830, Moving Average Loss= 0.769656, Moving Average Acc= 0.76938\n",
      "Iter 51000, Minibatch Loss= 0.380145, Training Accuracy= 0.38914, Moving Average Loss= 0.765761, Moving Average Acc= 0.76558\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-7639de1736c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m#Calculate batch accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y,\n\u001b[0;32m---> 50\u001b[0;31m                                                seqlen: batch_seqlen, ling_features: batch_LIWC_feats,  keep_prob:config.keep_prob})\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;31m#Calculate batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y,\n",
      "\u001b[0;32m/home/akoehler/anaconda2/envs/tflowtrial/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/akoehler/anaconda2/envs/tflowtrial/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/home/akoehler/anaconda2/envs/tflowtrial/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/akoehler/anaconda2/envs/tflowtrial/lib/python2.7/decimal.pyc\u001b[0m in \u001b[0;36m__float__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1578\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__floordiv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1580\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__float__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1581\u001b[0m         \u001b[0;34m\"\"\"Float representation.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_isnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    step = 1\n",
    "    moving_avg_loss = 0.0\n",
    "    moving_avg_acc = 0.0\n",
    "    discount = 0.01\n",
    "    batch_siz = 250\n",
    "\n",
    "    config.printConfiguration()\n",
    "    # print(type(batch_siz), type(config.training_iters))\n",
    "    # load DCAPS Data\n",
    "    \n",
    "    #dcaps_pdha_data, dcaps_pdha_label, dcaps_pdha_sequence_lengths, dcaps_pdha_sentences, _ = load_data(target=4, data_file_path='features/sequences/SimSensei_PDHA_fisherEmbeddings.csv')\n",
    "    #dcaps_frame_data, dcaps_frame_label, dcaps_frame_sequence_lengths, _, _ = load_data(target=4, data_file_path='features/sequences/SimSensei_Framing_fisherEmbeddings.csv')\n",
    "    #dcaps_wozai_data, dcaps_wozai_label, dcaps_wozai_sequence_lengths, dcaps_wozai_sentences, _ = load_data(target=4, data_file_path='features/sequences/WoZAI_fisherEmbeddings.csv')\n",
    "    \n",
    "    #dcaps_data, dcaps_label, dcaps_sequence_lengths, _ = load_data(target=4)\n",
    "    #dcaps_pdha_data, dcaps_pdha_label, dcaps_pdha_sequence_lengths, _ = load_data(target=4, data_file_path='features/sequences/SimSensei_PDHA.csv')\n",
    "    #dcaps_frame_data, dcaps_frame_label, dcaps_frame_sequence_lengths, _ = load_data(target=4, data_file_path='features/sequences/SimSensei_Framing.csv')\n",
    "    #dcaps_wozai_data, dcaps_wozai_label, dcaps_wozai_sequence_lengths, _ = load_data(target=4, data_file_path='features/sequences/WoZAI_corrected.csv')\n",
    "    # dcaps_bbn_data, dcaps_bbn_label, dcaps_bbn_sequence_lengths, _ = load_data(target=4, data_file_path='features/sequences/BBN.csv')\n",
    "    \n",
    "    #dcaps_data = dcaps_data + dcaps_frame_data + dcaps_pdha_data# + dcaps_pdha_data[:len(dcaps_pdha_data)/2]\n",
    "    #dcaps_label = dcaps_label + dcaps_frame_label + dcaps_pdha_label# + dcaps_pdha_label[:len(dcaps_pdha_label)/2]\n",
    "    #dcaps_sequence_lengths = dcaps_sequence_lengths + dcaps_frame_sequence_lengths + dcaps_pdha_sequence_lengths# + dcaps_pdha_sequence_lengths[:len(dcaps_pdha_sequence_lengths)/2]\n",
    "    print(len(dcaps_data), len(dcaps_label), len(dcaps_sequence_lengths))\n",
    "    valBatch_x, valBatch_y, valBatch_seqlen, valBatch_LIWC_feats, valBatch_sentences= random_batch(validation_x, validation_y, sequence_length_validation, LIWC_feats_validation, part_size=7500, sentences=utterances_validation) #sentences_validation\n",
    "    \n",
    "    #valBatch_x = dcaps_pdha_data[len(dcaps_pdha_data)/2:]\n",
    "    #valBatch_y = dcaps_pdha_label[len(dcaps_pdha_label)/2:]\n",
    "    #valBatch_seqlen = dcaps_pdha_sequence_lengths[len(dcaps_pdha_sequence_lengths)/2:]\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_siz < config.training_iters:\n",
    "        # batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "        # print(len(batch_x), len(batch_y), len(batch_seqlen))\n",
    "        batch_x, batch_y, batch_seqlen, batch_LIWC_feats, _ = random_batch(dcaps_data, dcaps_label, dcaps_sequence_lengths, LIWC_feats, part_size=batch_siz)\n",
    "        # print(batch_x[0], batch_y[0], batch_seqlen[0])\n",
    "        # print(batch_x[1], batch_y[1], len(batch_y[1]),  batch_seqlen[1])\n",
    "\n",
    "        # print(len(dcaps_sequence_lengths) , dcaps_sequence_lengths[0])\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, seqlen: batch_seqlen, ling_features: batch_LIWC_feats, keep_prob:config.keep_prob})\n",
    "        #sess.run(optimizer, feed_dict={x: dcaps_data, y: dcaps_label,\n",
    "        #                               seqlen: dcaps_sequence_lengths})\n",
    "        if step % config.display_step == 0:\n",
    "            #Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y,\n",
    "                                               seqlen: batch_seqlen, ling_features: batch_LIWC_feats,  keep_prob:config.keep_prob})\n",
    "            #Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y,\n",
    "                                            seqlen: batch_seqlen, ling_features: batch_LIWC_feats, keep_prob:config.keep_prob})\n",
    "\n",
    "            if step == 1:\n",
    "                moving_avg_loss = loss\n",
    "                moving_avg_acc = acc\n",
    "            else:\n",
    "                moving_avg_loss = (1.0-discount) * moving_avg_loss + discount * loss\n",
    "                moving_avg_acc = (1.0-discount) * moving_avg_acc + discount * acc\n",
    "            print(\"Iter \" + str(step*batch_siz) + \", Minibatch Loss= \" + \\\n",
    "                \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                \"{:.5f}\".format(acc)  + \", Moving Average Loss= \" + \\\n",
    "                \"{:.6f}\".format(moving_avg_loss) + \", Moving Average Acc= \" + \\\n",
    "                \"{:.5f}\".format(moving_avg_acc))\n",
    "        if step % config.val_step == 0:\n",
    "\n",
    "            print (\"Testing Accuracy:\", \\\n",
    "                sess.run(accuracy, feed_dict={x: valBatch_x, y: valBatch_y,\n",
    "                                              seqlen: valBatch_seqlen,ling_features: valBatch_LIWC_feats,  keep_prob:1}))\n",
    "            #print (\"Test Confmatrix: \\n\", \\\n",
    "            #sess.run(conf_mat, feed_dict={x: valBatch_x, y: valBatch_y, seqlen: valBatch_seqlen,ling_features: valBatch_LIWC_feats , keep_prob:1}))\n",
    "            val_predictions = sess.run(pred, feed_dict={x: valBatch_x, y: valBatch_y, seqlen: valBatch_seqlen,ling_features: valBatch_LIWC_feats ,  keep_prob:1}) #get predicted classes for validation batch\n",
    "            #val_prob_predictions = sess.run(prob, feed_dict={x: valBatch_x, y: valBatch_y, seqlen: valBatch_seqlen, ling_features: valBatch_LIWC_feats , keep_prob:1}) #get predicted probabilities for validation batch\n",
    "            if config.debug_sentences: \n",
    "                write_debug_sentences(valBatch_y, step*batch_siz, val_predictions)\n",
    "            \n",
    "        step += 1\n",
    "    print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
